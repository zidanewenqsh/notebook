{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'  #默认为'last'\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from numpy import linalg\n",
    "np.set_printoptions(threshold=np.inf, precision=2, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'weight'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'bias'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = nn.Linear(2,3,bias=True)\n",
    "for n,v in l1.named_parameters():\n",
    "    n\n",
    "    v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0248,  0.5489],\n",
       "        [ 1.3520, -2.0637],\n",
       "        [-0.9140,  1.1142]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.2908, 0.9804, 0.0706])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(3,2)\n",
    "b = torch.randn(3,)\n",
    "a\n",
    "b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.nn.functional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### interpolate  \n",
    "`torch.nn.functional.interpolate(input, size=None, scale_factor=None, mode='nearest', align_corners=None)`  \n",
    "Parameters\n",
    "* input (Tensor) – the input tensor\n",
    "\n",
    "* size (python:int or Tuple[python:int] or Tuple[python:int, python:int] or Tuple[python:int, python:int, python:int]) – output spatial size.\n",
    "\n",
    "* scale_factor (python:float or Tuple[python:float]) – multiplier for spatial size. Has to match input size if it is a tuple.\n",
    "\n",
    "* mode (str) – algorithm used for upsampling: 'nearest' | 'linear' | 'bilinear' | 'bicubic' | 'trilinear' | 'area'. Default: 'nearest'\n",
    "\n",
    "* align_corners (bool, optional) – Geometrically, we consider the pixels of the input and output as squares rather than points. If set to True, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to False, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when scale_factor is kept the same. This only has an effect when mode is 'linear', 'bilinear', 'bicubic' or 'trilinear'. Default: False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 1.],\n",
       "         [2., 3.]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 1., 1.],\n",
       "         [2., 2., 3., 3.]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 1.],\n",
       "          [2., 3.]],\n",
       "\n",
       "         [[4., 5.],\n",
       "          [6., 7.]]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 1., 1.],\n",
       "          [0., 0., 1., 1.],\n",
       "          [2., 2., 3., 3.],\n",
       "          [2., 2., 3., 3.]],\n",
       "\n",
       "         [[4., 4., 5., 5.],\n",
       "          [4., 4., 5., 5.],\n",
       "          [6., 6., 7., 7.],\n",
       "          [6., 6., 7., 7.]]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_printoptions(precision=2)\n",
    "a=torch.arange(4.).reshape(1,2,2)\n",
    "a\n",
    "nn.functional.interpolate(a,scale_factor=2,mode='nearest')\n",
    "a=torch.arange(8.).reshape(1,2,2,2)\n",
    "a\n",
    "nn.functional.interpolate(a,scale_factor=2,mode='nearest')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.,  2.,  3.],\n",
       "          [ 4.,  5.,  6.],\n",
       "          [ 7.,  8.,  9.]],\n",
       "\n",
       "         [[10., 11., 12.],\n",
       "          [13., 14., 15.],\n",
       "          [16., 17., 18.]]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.00,  1.40,  1.80,  2.20,  2.60,  3.00],\n",
       "          [ 2.20,  2.60,  3.00,  3.40,  3.80,  4.20],\n",
       "          [ 3.40,  3.80,  4.20,  4.60,  5.00,  5.40],\n",
       "          [ 4.60,  5.00,  5.40,  5.80,  6.20,  6.60],\n",
       "          [ 5.80,  6.20,  6.60,  7.00,  7.40,  7.80],\n",
       "          [ 7.00,  7.40,  7.80,  8.20,  8.60,  9.00]],\n",
       "\n",
       "         [[10.00, 10.40, 10.80, 11.20, 11.60, 12.00],\n",
       "          [11.20, 11.60, 12.00, 12.40, 12.80, 13.20],\n",
       "          [12.40, 12.80, 13.20, 13.60, 14.00, 14.40],\n",
       "          [13.60, 14.00, 14.40, 14.80, 15.20, 15.60],\n",
       "          [14.80, 15.20, 15.60, 16.00, 16.40, 16.80],\n",
       "          [16.00, 16.40, 16.80, 17.20, 17.60, 18.00]]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.00,  1.25,  1.75,  2.25,  2.75,  3.00],\n",
       "          [ 1.75,  2.00,  2.50,  3.00,  3.50,  3.75],\n",
       "          [ 3.25,  3.50,  4.00,  4.50,  5.00,  5.25],\n",
       "          [ 4.75,  5.00,  5.50,  6.00,  6.50,  6.75],\n",
       "          [ 6.25,  6.50,  7.00,  7.50,  8.00,  8.25],\n",
       "          [ 7.00,  7.25,  7.75,  8.25,  8.75,  9.00]],\n",
       "\n",
       "         [[10.00, 10.25, 10.75, 11.25, 11.75, 12.00],\n",
       "          [10.75, 11.00, 11.50, 12.00, 12.50, 12.75],\n",
       "          [12.25, 12.50, 13.00, 13.50, 14.00, 14.25],\n",
       "          [13.75, 14.00, 14.50, 15.00, 15.50, 15.75],\n",
       "          [15.25, 15.50, 16.00, 16.50, 17.00, 17.25],\n",
       "          [16.00, 16.25, 16.75, 17.25, 17.75, 18.00]]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_printoptions(precision=2)\n",
    "a=torch.arange(8.).reshape(1,2,2,2)\n",
    "a=torch.arange(1,19.).reshape(1,2,3,3)\n",
    "a\n",
    "nn.functional.interpolate(a,scale_factor=2,mode='bilinear',align_corners=True)\n",
    "nn.functional.interpolate(a,scale_factor=2,mode='bilinear',align_corners=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[一文看懂align_corners](https://zhuanlan.zhihu.com/p/87572724)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### upsample(废弃，改用interpolate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 3.],\n",
       "        [4., 0., 5.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = torch.LongTensor([[0, 1, 1], [2, 0, 2]])\n",
    "v = torch.FloatTensor([3, 4, 5])\n",
    "torch.sparse.FloatTensor(i, v, torch.Size([2,3])).to_dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 3.],\n",
       "        [4., 0., 5.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = torch.LongTensor([[0, 2], [1, 0], [1, 2]])\n",
    "v = torch.FloatTensor([3, 4, 5])\n",
    "torch.sparse.FloatTensor(i.t(), v, torch.Size([2, 3])).to_dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 4]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 3.],\n",
       "        [5., 7.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[2, 4]]),\n",
       "       values=tensor([[1., 3.],\n",
       "                      [5., 7.]]),\n",
       "       size=(5, 2), nnz=2, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [1., 3.],\n",
       "        [0., 0.],\n",
       "        [5., 7.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = torch.LongTensor([[2, 4]])\n",
    "i\n",
    "v = torch.FloatTensor([[1, 3], [5, 7]])\n",
    "v\n",
    "torch.sparse.FloatTensor(i, v)\n",
    "torch.sparse.FloatTensor(i, v).to_dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[0, 0, 0, 1, 1, 1],\n",
       "                       [0, 1, 2, 0, 1, 2]]),\n",
       "       values=tensor([ 2.0876, -0.3764,  0.4160, -0.5974, -1.8790,  1.1513]),\n",
       "       size=(2, 3), nnz=6, layout=torch.sparse_coo, requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.0876, -0.3764,  0.4160],\n",
       "        [-0.5974, -1.8790,  1.1513]], grad_fn=<ToDenseBackward>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.1082,  0.7027],\n",
       "        [ 1.6664,  0.8173],\n",
       "        [-1.6953,  0.5092]], requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.0686,  1.3711],\n",
       "        [-6.3424, -1.3692]], grad_fn=<SparseAddmmBackward>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[0, 0, 0, 1, 1, 1],\n",
       "                       [0, 1, 2, 0, 1, 2]]),\n",
       "       values=tensor([ 2.8109,  2.4837, -1.1860,  2.8109,  2.4837, -1.1860]),\n",
       "       size=(2, 3), nnz=6, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(2, 3).to_sparse().requires_grad_(True)\n",
    "a\n",
    "a.to_dense()\n",
    "# tensor(indices=tensor([[0, 0, 0, 1, 1, 1],\n",
    "#                        [0, 1, 2, 0, 1, 2]]),\n",
    "#        values=tensor([ 1.5901,  0.0183, -0.6146,  1.8061, -0.0112,  0.6302]),\n",
    "#        size=(2, 3), nnz=6, layout=torch.sparse_coo, requires_grad=True)\n",
    "\n",
    "b = torch.randn(3, 2, requires_grad=True)\n",
    "b\n",
    "# tensor([[-0.6479,  0.7874],\n",
    "#         [-1.2056,  0.5641],\n",
    "#         [-1.1716, -0.9923]], requires_grad=True)\n",
    "\n",
    "y = torch.sparse.mm(a, b)\n",
    "y\n",
    "# tensor([[-0.3323,  1.8723],\n",
    "#         [-1.8951,  0.7904]], grad_fn=<SparseAddmmBackward>)\n",
    "y.sum().backward()\n",
    "a.grad\n",
    "# tensor(indices=tensor([[0, 0, 0, 1, 1, 1],\n",
    "#                        [0, 1, 2, 0, 1, 2]]),\n",
    "#        values=tensor([ 0.1394, -0.6415, -2.1639,  0.1394, -0.6415, -2.1639]),\n",
    "#        size=(2, 3), nnz=6, layout=torch.sparse_coo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 2.]]), tensor([[3., 4.]]), tensor([[5., 6.]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.],\n",
       "        [5., 6.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1=torch.Tensor([[1,2]])\n",
    "a2=torch.Tensor([[3,4]])\n",
    "a3=torch.Tensor([[5,6]])\n",
    "a = (a1,a2,a3)\n",
    "a\n",
    "torch.cat(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3010,  0.1340, -2.2113, -0.6608],\n",
       "        [ 0.7092,  0.0731, -1.2054, -0.3602],\n",
       "        [-0.5546, -0.0571,  0.9426,  0.2817],\n",
       "        [ 0.6613,  0.0681, -1.1240, -0.3359],\n",
       "        [ 0.6079,  0.0626, -1.0333, -0.3088]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6743,  0.2117, -0.1060],\n",
       "        [-5.9914,  2.7196, -8.2246]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9479, -0.8942,  0.6105,  1.2177],\n",
       "         [ 5.1920, -2.0442,  3.7597,  0.2774]],\n",
       "\n",
       "        [[ 1.9755,  3.0665,  0.5966, -2.7625],\n",
       "         [-1.7776, -4.3762, -0.1269, -1.9066]],\n",
       "\n",
       "        [[ 1.1460,  1.0215, -0.8949,  1.3821],\n",
       "         [-0.1486, -1.9964,  1.4362, -0.9691]]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.5741,  0.7686, -1.5141])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9746, -1.4177, -1.3442],\n",
       "        [-0.2116,  0.5845,  0.7181],\n",
       "        [ 0.2219, -0.7531,  1.5207],\n",
       "        [-0.9117, -1.1787,  0.6000]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 5, 4])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(5)\n",
    "y = torch.randn(4)\n",
    "torch.einsum('i,j->ij', x, y)  # outer product\n",
    "# tensor([[-0.0570, -0.0286, -0.0231,  0.0197],\n",
    "#         [ 1.2616,  0.6335,  0.5113, -0.4351],\n",
    "#         [ 1.4452,  0.7257,  0.5857, -0.4984],\n",
    "#         [-0.4647, -0.2333, -0.1883,  0.1603],\n",
    "#         [-1.1130, -0.5588, -0.4510,  0.3838]])\n",
    "\n",
    "\n",
    "A = torch.randn(3,5,4)\n",
    "l = torch.randn(2,5)\n",
    "r = torch.randn(2,4)\n",
    "torch.einsum('bn,anm,bm->ba', l, A, r) # compare torch.nn.functional.bilinear\n",
    "# tensor([[-0.3430, -5.2405,  0.4494],\n",
    "#         [ 0.3311,  5.5201, -3.0356]])\n",
    "\n",
    "\n",
    "As = torch.randn(3,2,5)\n",
    "Bs = torch.randn(3,5,4)\n",
    "torch.einsum('bij,bjk->bik', As, Bs) # batch matrix multiplication\n",
    "# tensor([[[-1.0564, -1.5904,  3.2023,  3.1271],\n",
    "#          [-1.6706, -0.8097, -0.8025, -2.1183]],\n",
    "\n",
    "#         [[ 4.2239,  0.3107, -0.5756, -0.2354],\n",
    "#          [-1.4558, -0.3460,  1.5087, -0.8530]],\n",
    "\n",
    "#         [[ 2.8153,  1.8787, -4.3839, -1.2112],\n",
    "#          [ 0.3728, -2.1131,  0.0921,  0.8305]]])\n",
    "\n",
    "A = torch.randn(3, 3)\n",
    "torch.einsum('ii->i', A) # diagonal\n",
    "# tensor([-0.7825,  0.8291, -0.1936])\n",
    "\n",
    "A = torch.randn(4, 3, 3)\n",
    "torch.einsum('...ii->...i', A) # batch diagonal\n",
    "# tensor([[-1.0864,  0.7292,  0.0569],\n",
    "#         [-0.9725, -1.0270,  0.6493],\n",
    "#         [ 0.5832, -1.1716, -1.5084],\n",
    "#         [ 0.4041, -1.1690,  0.8570]])\n",
    "\n",
    "A = torch.randn(2, 3, 4, 5)\n",
    "torch.einsum('...ij->...ji', A).shape # batch permute\n",
    "# torch.Size([2, 3, 5, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1.],\n",
       "         [1., 1.]],\n",
       "\n",
       "        [[1., 1.],\n",
       "         [1., 1.]],\n",
       "\n",
       "        [[1., 1.],\n",
       "         [1., 1.]]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 6.,  6.],\n",
       "          [ 6.,  6.]],\n",
       "\n",
       "         [[22., 22.],\n",
       "          [22., 22.]],\n",
       "\n",
       "         [[38., 38.],\n",
       "          [38., 38.]]],\n",
       "\n",
       "\n",
       "        [[[54., 54.],\n",
       "          [54., 54.]],\n",
       "\n",
       "         [[70., 70.],\n",
       "          [70., 70.]],\n",
       "\n",
       "         [[86., 86.],\n",
       "          [86., 86.]]]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 2, 2])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(24).reshape(2,3,4)\n",
    "b = torch.arange(12).reshape(3,2,2)\n",
    "b = torch.ones(3,2,2)\n",
    "a\n",
    "b\n",
    "# torch.einsum('bij,bjk', a, b）\n",
    "c = torch.einsum('ij...,jkl->ijkl', a, b)\n",
    "c\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  1.],\n",
       "          [ 2.,  3.],\n",
       "          [ 4.,  5.]],\n",
       "\n",
       "         [[ 6.,  7.],\n",
       "          [ 8.,  9.],\n",
       "          [10., 11.]]],\n",
       "\n",
       "\n",
       "        [[[12., 13.],\n",
       "          [14., 15.],\n",
       "          [16., 17.]],\n",
       "\n",
       "         [[18., 19.],\n",
       "          [20., 21.],\n",
       "          [22., 23.]]]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 2., 1., 2.],\n",
       "         [1., 2., 1., 2.],\n",
       "         [1., 2., 1., 2.]],\n",
       "\n",
       "        [[1., 2., 1., 2.],\n",
       "         [1., 2., 1., 2.],\n",
       "         [1., 2., 1., 2.]]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.,  2.,  1.,  2.],\n",
       "          [ 5., 10.,  5., 10.],\n",
       "          [ 9., 18.,  9., 18.]],\n",
       "\n",
       "         [[13., 26., 13., 26.],\n",
       "          [17., 34., 17., 34.],\n",
       "          [21., 42., 21., 42.]]],\n",
       "\n",
       "\n",
       "        [[[25., 50., 25., 50.],\n",
       "          [29., 58., 29., 58.],\n",
       "          [33., 66., 33., 66.]],\n",
       "\n",
       "         [[37., 74., 37., 74.],\n",
       "          [41., 82., 41., 82.],\n",
       "          [45., 90., 45., 90.]]]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 4])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(24.).reshape(2,2,3,2)\n",
    "# b = torch.arange(12).reshape(3,2,2)\n",
    "b = torch.Tensor([1,2]).repeat(2, 3, 2)\n",
    "a\n",
    "b\n",
    "c = torch.einsum('bij...,bjk->bijk', a, b)\n",
    "# c = torch.einsum('ij,jk->ik', a, b)\n",
    "c\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function repeat:\n",
      "\n",
      "repeat(...) method of torch.Tensor instance\n",
      "    repeat(*sizes) -> Tensor\n",
      "    \n",
      "    Repeats this tensor along the specified dimensions.\n",
      "    \n",
      "    Unlike :meth:`~Tensor.expand`, this function copies the tensor's data.\n",
      "    \n",
      "    .. warning::\n",
      "    \n",
      "        :func:`torch.repeat` behaves differently from\n",
      "        `numpy.repeat <https://docs.scipy.org/doc/numpy/reference/generated/numpy.repeat.html>`_,\n",
      "        but is more similar to\n",
      "        `numpy.tile <https://docs.scipy.org/doc/numpy/reference/generated/numpy.tile.html>`_.\n",
      "        For the operator similar to `numpy.repeat`, see :func:`torch.repeat_interleave`.\n",
      "    \n",
      "    Args:\n",
      "        sizes (torch.Size or int...): The number of times to repeat this tensor along each\n",
      "            dimension\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> x = torch.tensor([1, 2, 3])\n",
      "        >>> x.repeat(4, 2)\n",
      "        tensor([[ 1,  2,  3,  1,  2,  3],\n",
      "                [ 1,  2,  3,  1,  2,  3],\n",
      "                [ 1,  2,  3,  1,  2,  3],\n",
      "                [ 1,  2,  3,  1,  2,  3]])\n",
      "        >>> x.repeat(4, 2, 1).size()\n",
      "        torch.Size([4, 2, 3])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(a.repeat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prune\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square conv kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5x5 image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, int(x.nelement() / x.shape[0]))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = LeNet().to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('weight', Parameter containing:\n",
      "tensor([[[[ 0.0676,  0.2664, -0.1045],\n",
      "          [-0.0686,  0.0935,  0.2987],\n",
      "          [ 0.3007,  0.2398,  0.1515]]],\n",
      "\n",
      "\n",
      "        [[[-0.2550, -0.0591, -0.1830],\n",
      "          [ 0.0049, -0.2507, -0.1184],\n",
      "          [-0.1625, -0.1486, -0.1400]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2331, -0.2785, -0.1710],\n",
      "          [ 0.0477, -0.2853,  0.2136],\n",
      "          [-0.3326,  0.2331, -0.3113]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1029, -0.0356, -0.2184],\n",
      "          [ 0.1442,  0.1608, -0.2759],\n",
      "          [ 0.2694, -0.1319, -0.1092]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0256, -0.0950, -0.1283],\n",
      "          [ 0.1312, -0.3005,  0.2354],\n",
      "          [-0.0945,  0.0965,  0.0685]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2051,  0.2528, -0.0950],\n",
      "          [-0.2479,  0.2989, -0.0678],\n",
      "          [-0.0997,  0.1652,  0.1718]]]], requires_grad=True)), ('bias', Parameter containing:\n",
      "tensor([-0.1904,  0.1956,  0.0284, -0.1526, -0.1712,  0.3311],\n",
      "       requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "module = model.conv1\n",
    "print(list(module.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(list(module.named_buffers()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prune.random_unstructured(module, name=\"weight\", amount=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bias', Parameter containing:\n",
      "tensor([-0.1904,  0.1956,  0.0284, -0.1526, -0.1712,  0.3311],\n",
      "       requires_grad=True)), ('weight_orig', Parameter containing:\n",
      "tensor([[[[ 0.0676,  0.2664, -0.1045],\n",
      "          [-0.0686,  0.0935,  0.2987],\n",
      "          [ 0.3007,  0.2398,  0.1515]]],\n",
      "\n",
      "\n",
      "        [[[-0.2550, -0.0591, -0.1830],\n",
      "          [ 0.0049, -0.2507, -0.1184],\n",
      "          [-0.1625, -0.1486, -0.1400]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2331, -0.2785, -0.1710],\n",
      "          [ 0.0477, -0.2853,  0.2136],\n",
      "          [-0.3326,  0.2331, -0.3113]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1029, -0.0356, -0.2184],\n",
      "          [ 0.1442,  0.1608, -0.2759],\n",
      "          [ 0.2694, -0.1319, -0.1092]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0256, -0.0950, -0.1283],\n",
      "          [ 0.1312, -0.3005,  0.2354],\n",
      "          [-0.0945,  0.0965,  0.0685]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2051,  0.2528, -0.0950],\n",
      "          [-0.2479,  0.2989, -0.0678],\n",
      "          [-0.0997,  0.1652,  0.1718]]]], requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "print(list(module.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('weight_mask', tensor([[[[0., 1., 1.],\n",
      "          [0., 1., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 1., 1.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0.],\n",
      "          [0., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [0., 1., 1.],\n",
      "          [0., 0., 0.]]]]))]\n"
     ]
    }
   ],
   "source": [
    "print(list(module.named_buffers()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.0000,  0.2664, -0.1045],\n",
      "          [-0.0000,  0.0935,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.2550, -0.0591, -0.1830],\n",
      "          [ 0.0049, -0.0000, -0.0000],\n",
      "          [-0.1625, -0.0000, -0.1400]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0000, -0.2853,  0.2136],\n",
      "          [-0.0000,  0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1029, -0.0356, -0.0000],\n",
      "          [ 0.0000,  0.1608, -0.2759],\n",
      "          [ 0.2694, -0.1319, -0.1092]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0256, -0.0950, -0.1283],\n",
      "          [ 0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.0965,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2051,  0.2528, -0.0950],\n",
      "          [-0.0000,  0.2989, -0.0678],\n",
      "          [-0.0000,  0.0000,  0.0000]]]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(module.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([(1, <torch.nn.utils.prune.PruningContainer object at 0x7fc11290afa0>)])\n"
     ]
    }
   ],
   "source": [
    "print(module._forward_pre_hooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prune.l1_unstructured(module, name=\"bias\", amount=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('weight_orig', Parameter containing:\n",
      "tensor([[[[ 0.0676,  0.2664, -0.1045],\n",
      "          [-0.0686,  0.0935,  0.2987],\n",
      "          [ 0.3007,  0.2398,  0.1515]]],\n",
      "\n",
      "\n",
      "        [[[-0.2550, -0.0591, -0.1830],\n",
      "          [ 0.0049, -0.2507, -0.1184],\n",
      "          [-0.1625, -0.1486, -0.1400]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2331, -0.2785, -0.1710],\n",
      "          [ 0.0477, -0.2853,  0.2136],\n",
      "          [-0.3326,  0.2331, -0.3113]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1029, -0.0356, -0.2184],\n",
      "          [ 0.1442,  0.1608, -0.2759],\n",
      "          [ 0.2694, -0.1319, -0.1092]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0256, -0.0950, -0.1283],\n",
      "          [ 0.1312, -0.3005,  0.2354],\n",
      "          [-0.0945,  0.0965,  0.0685]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2051,  0.2528, -0.0950],\n",
      "          [-0.2479,  0.2989, -0.0678],\n",
      "          [-0.0997,  0.1652,  0.1718]]]], requires_grad=True)), ('bias_orig', Parameter containing:\n",
      "tensor([-0.1904,  0.1956,  0.0284, -0.1526, -0.1712,  0.3311],\n",
      "       requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "print(list(module.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('weight_mask', tensor([[[[0., 1., 1.],\n",
      "          [0., 1., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 1., 1.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0.],\n",
      "          [0., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [0., 1., 1.],\n",
      "          [0., 0., 0.]]]])), ('bias_mask', tensor([1., 1., 0., 0., 0., 1.]))]\n"
     ]
    }
   ],
   "source": [
    "print(list(module.named_buffers()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1904,  0.1956,  0.0000, -0.0000, -0.0000,  0.3311],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(module.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([(1, <torch.nn.utils.prune.PruningContainer object at 0x7fc11290afa0>), (2, <torch.nn.utils.prune.L1Unstructured object at 0x7fc1128fcd60>)])\n"
     ]
    }
   ],
   "source": [
    "print(module._forward_pre_hooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.2550, -0.0591, -0.1830],\n",
      "          [ 0.0049, -0.0000, -0.0000],\n",
      "          [-0.1625, -0.0000, -0.1400]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1029, -0.0356, -0.0000],\n",
      "          [ 0.0000,  0.1608, -0.2759],\n",
      "          [ 0.2694, -0.1319, -0.1092]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2051,  0.2528, -0.0950],\n",
      "          [-0.0000,  0.2989, -0.0678],\n",
      "          [-0.0000,  0.0000,  0.0000]]]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "prune.ln_structured(module, name=\"weight\", amount=0.5, n=2, dim=0)\n",
    "\n",
    "# As we can verify, this will zero out all the connections corresponding to\n",
    "# 50% (3 out of 6) of the channels, while preserving the action of the\n",
    "# previous mask.\n",
    "print(module.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<torch.nn.utils.prune.RandomUnstructured object at 0x7fc1128fc610>, <torch.nn.utils.prune.RandomUnstructured object at 0x7fc11290ad00>, <torch.nn.utils.prune.LnStructured object at 0x7fc11290a910>]\n"
     ]
    }
   ],
   "source": [
    "for hook in module._forward_pre_hooks.values():\n",
    "    if hook._tensor_name == \"weight\":  # select out the correct hook\n",
    "        break\n",
    "\n",
    "print(list(hook))  # pruning history in the container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serializing a pruned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['conv1.weight_orig', 'conv1.bias_orig', 'conv1.weight_mask', 'conv1.bias_mask', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('weight_orig', Parameter containing:\n",
      "tensor([[[[ 0.0676,  0.2664, -0.1045],\n",
      "          [-0.0686,  0.0935,  0.2987],\n",
      "          [ 0.3007,  0.2398,  0.1515]]],\n",
      "\n",
      "\n",
      "        [[[-0.2550, -0.0591, -0.1830],\n",
      "          [ 0.0049, -0.2507, -0.1184],\n",
      "          [-0.1625, -0.1486, -0.1400]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2331, -0.2785, -0.1710],\n",
      "          [ 0.0477, -0.2853,  0.2136],\n",
      "          [-0.3326,  0.2331, -0.3113]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1029, -0.0356, -0.2184],\n",
      "          [ 0.1442,  0.1608, -0.2759],\n",
      "          [ 0.2694, -0.1319, -0.1092]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0256, -0.0950, -0.1283],\n",
      "          [ 0.1312, -0.3005,  0.2354],\n",
      "          [-0.0945,  0.0965,  0.0685]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2051,  0.2528, -0.0950],\n",
      "          [-0.2479,  0.2989, -0.0678],\n",
      "          [-0.0997,  0.1652,  0.1718]]]], requires_grad=True)), ('bias_orig', Parameter containing:\n",
      "tensor([-0.1904,  0.1956,  0.0284, -0.1526, -0.1712,  0.3311],\n",
      "       requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "print(list(module.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('weight_mask', tensor([[[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0.],\n",
      "          [0., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [0., 1., 1.],\n",
      "          [0., 0., 0.]]]])), ('bias_mask', tensor([1., 1., 0., 0., 0., 1.]))]\n"
     ]
    }
   ],
   "source": [
    "print(list(module.named_buffers()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Parameter 'weight' of module Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1)) has to be pruned before pruning can be removed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-a684cda40fb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprune\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'weight'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/utils/prune.py\u001b[0m in \u001b[0;36mremove\u001b[0;34m(module, name)\u001b[0m\n\u001b[1;32m   1122\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m     raise ValueError(\n\u001b[0m\u001b[1;32m   1125\u001b[0m         \u001b[0;34m\"Parameter '{}' of module {} has to be pruned \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m         \u001b[0;34m\"before pruning can be removed\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Parameter 'weight' of module Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1)) has to be pruned before pruning can be removed"
     ]
    }
   ],
   "source": [
    "prune.remove(module, 'weight')\n",
    "print(list(module.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bias_mask', tensor([1., 1., 0., 0., 0., 1.]))]\n"
     ]
    }
   ],
   "source": [
    "print(list(module.named_buffers()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning multiple parameters in a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model = LeNet()\n",
    "# new_model.save(\"./save/a.pt\")\n",
    "torch.save(new_model, \"./save/a.pt\")\n",
    "# new_model2 = new_model.clone()\n",
    "# 400*120\n",
    "a = torch.randn(1,1,28,28)\n",
    "\n",
    "b = new_model(a)\n",
    "b.shape\n",
    "# for n, p in new_model.named_parameters():\n",
    "#     n, p.shape\n",
    "# for n, p in new_model.named_modules():\n",
    "#     n\n",
    "#     p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "607 µs ± 49 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "b = new_model(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Linear(in_features=400, out_features=120, bias=True)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Linear(in_features=120, out_features=84, bias=True)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Linear(in_features=84, out_features=10, bias=True)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['conv1.weight_mask', 'conv2.weight_mask', 'fc1.weight_mask', 'fc2.weight_mask', 'fc3.weight_mask'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for name, module in new_model.named_modules():\n",
    "    # prune 20% of connections in all 2D-conv layers\n",
    "    if isinstance(module, torch.nn.Conv2d):\n",
    "        prune.l1_unstructured(module, name='weight', amount=0.6)\n",
    "    # prune 40% of connections in all linear layers\n",
    "    elif isinstance(module, torch.nn.Linear):\n",
    "        prune.l1_unstructured(module, name='weight', amount=0.4)\n",
    "\n",
    "print(dict(new_model.named_buffers()).keys())  # to verify that all masks exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('conv1.bias', Parameter containing:\n",
      "tensor([-0.2661, -0.1464,  0.1981, -0.2879,  0.1437,  0.3165],\n",
      "       requires_grad=True)), ('conv1.weight_orig', Parameter containing:\n",
      "tensor([[[[-0.0526, -0.1373,  0.2569],\n",
      "          [-0.0020, -0.2266,  0.2198],\n",
      "          [-0.3097,  0.1792, -0.3308]]],\n",
      "\n",
      "\n",
      "        [[[-0.3077,  0.0166,  0.1194],\n",
      "          [ 0.1629,  0.1295,  0.0457],\n",
      "          [ 0.2685, -0.0435, -0.1927]]],\n",
      "\n",
      "\n",
      "        [[[-0.0340, -0.0738, -0.1328],\n",
      "          [-0.1415, -0.2674,  0.0236],\n",
      "          [ 0.2277, -0.0227,  0.1259]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1886,  0.0692,  0.1732],\n",
      "          [ 0.2386, -0.0198, -0.2122],\n",
      "          [-0.2701,  0.0327, -0.2260]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3324, -0.0460,  0.2436],\n",
      "          [-0.1378, -0.3200,  0.1307],\n",
      "          [-0.1458, -0.0168,  0.0722]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1197, -0.0244, -0.0780],\n",
      "          [ 0.2695,  0.2498,  0.2103],\n",
      "          [-0.1643, -0.0130, -0.2759]]]], requires_grad=True)), ('conv2.bias', Parameter containing:\n",
      "tensor([-0.0796,  0.1039, -0.0753,  0.0784,  0.0028, -0.0838,  0.0203, -0.1073,\n",
      "        -0.0808,  0.0832,  0.0328, -0.0539, -0.0812, -0.0558, -0.1227, -0.0148],\n",
      "       requires_grad=True)), ('conv2.weight_orig', Parameter containing:\n",
      "tensor([[[[ 0.1349,  0.1354, -0.1260],\n",
      "          [-0.0937, -0.0808, -0.0918],\n",
      "          [ 0.1005, -0.0209,  0.0839]],\n",
      "\n",
      "         [[ 0.0833,  0.1240, -0.0391],\n",
      "          [-0.0667,  0.0037,  0.0028],\n",
      "          [-0.0809,  0.1147,  0.0087]],\n",
      "\n",
      "         [[-0.0790,  0.0452,  0.0417],\n",
      "          [ 0.0788,  0.1181, -0.1241],\n",
      "          [-0.0281,  0.1183, -0.1225]],\n",
      "\n",
      "         [[-0.0898,  0.0944, -0.0411],\n",
      "          [-0.0139,  0.0743,  0.0829],\n",
      "          [-0.0081,  0.1006, -0.1341]],\n",
      "\n",
      "         [[-0.0412,  0.0100,  0.0936],\n",
      "          [-0.0773,  0.1198,  0.0764],\n",
      "          [-0.0310,  0.0460, -0.1111]],\n",
      "\n",
      "         [[-0.0273,  0.0619,  0.0646],\n",
      "          [ 0.0789,  0.0779,  0.0293],\n",
      "          [-0.0058,  0.0930,  0.0179]]],\n",
      "\n",
      "\n",
      "        [[[-0.0346, -0.0415, -0.0808],\n",
      "          [-0.0965,  0.1013, -0.1208],\n",
      "          [-0.1030, -0.1001, -0.1264]],\n",
      "\n",
      "         [[ 0.0304,  0.0065, -0.1206],\n",
      "          [ 0.1029, -0.0895, -0.0036],\n",
      "          [ 0.0868, -0.1139,  0.1165]],\n",
      "\n",
      "         [[-0.0404,  0.0624,  0.1309],\n",
      "          [ 0.0016, -0.0009,  0.0331],\n",
      "          [ 0.1065,  0.1268,  0.0682]],\n",
      "\n",
      "         [[ 0.1027, -0.1338,  0.0617],\n",
      "          [-0.0229,  0.0227, -0.1179],\n",
      "          [-0.1096, -0.1007, -0.1228]],\n",
      "\n",
      "         [[-0.0290,  0.0306, -0.0927],\n",
      "          [ 0.0484,  0.0165,  0.0082],\n",
      "          [-0.0525,  0.0402,  0.0190]],\n",
      "\n",
      "         [[ 0.1301, -0.0667, -0.0896],\n",
      "          [-0.1308, -0.1270,  0.0097],\n",
      "          [ 0.1082, -0.0102,  0.0220]]],\n",
      "\n",
      "\n",
      "        [[[-0.0213,  0.0120,  0.0408],\n",
      "          [ 0.0421,  0.0820, -0.0948],\n",
      "          [ 0.0679,  0.0150, -0.0581]],\n",
      "\n",
      "         [[ 0.1139, -0.0404, -0.0465],\n",
      "          [ 0.0689,  0.0052, -0.0198],\n",
      "          [-0.0875, -0.1078,  0.0898]],\n",
      "\n",
      "         [[-0.0677,  0.0859, -0.1066],\n",
      "          [ 0.1174,  0.1088,  0.0483],\n",
      "          [ 0.0120, -0.0744, -0.0870]],\n",
      "\n",
      "         [[-0.0699, -0.0915,  0.1024],\n",
      "          [ 0.0534,  0.1187, -0.0809],\n",
      "          [-0.0403,  0.1334,  0.1196]],\n",
      "\n",
      "         [[-0.0507,  0.0787,  0.0292],\n",
      "          [ 0.1062,  0.0349,  0.0976],\n",
      "          [-0.1327,  0.0968, -0.1019]],\n",
      "\n",
      "         [[ 0.0818, -0.0308, -0.0287],\n",
      "          [ 0.0970, -0.1188, -0.0349],\n",
      "          [ 0.0136,  0.0776, -0.0623]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0890, -0.0650,  0.0438],\n",
      "          [ 0.0642,  0.1191,  0.0408],\n",
      "          [-0.1102,  0.1138, -0.0054]],\n",
      "\n",
      "         [[ 0.0369, -0.0681, -0.0420],\n",
      "          [-0.1343, -0.0598, -0.1279],\n",
      "          [-0.0073,  0.0075,  0.0818]],\n",
      "\n",
      "         [[-0.0028, -0.0859,  0.1133],\n",
      "          [ 0.0231,  0.0865, -0.0477],\n",
      "          [-0.0803, -0.0471,  0.0272]],\n",
      "\n",
      "         [[-0.0585,  0.0122,  0.0912],\n",
      "          [-0.0073, -0.1070,  0.0452],\n",
      "          [ 0.0231, -0.0286, -0.0799]],\n",
      "\n",
      "         [[ 0.1080, -0.0237, -0.0313],\n",
      "          [ 0.0374,  0.1116,  0.0502],\n",
      "          [ 0.0962, -0.0258,  0.0667]],\n",
      "\n",
      "         [[ 0.0633,  0.0528,  0.0945],\n",
      "          [ 0.0862,  0.0435, -0.1340],\n",
      "          [-0.1125, -0.0136, -0.1036]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1270,  0.0524,  0.0422],\n",
      "          [-0.1180, -0.0506, -0.1143],\n",
      "          [ 0.0665, -0.0356,  0.1132]],\n",
      "\n",
      "         [[-0.0488,  0.1132, -0.0098],\n",
      "          [ 0.0545,  0.1073,  0.1336],\n",
      "          [-0.0945, -0.0265,  0.1203]],\n",
      "\n",
      "         [[-0.0724,  0.0470,  0.0949],\n",
      "          [ 0.1086,  0.0229,  0.1113],\n",
      "          [ 0.0473,  0.0866,  0.0331]],\n",
      "\n",
      "         [[-0.1303,  0.1106,  0.0933],\n",
      "          [ 0.0906,  0.1321,  0.0043],\n",
      "          [-0.0078, -0.0753,  0.0326]],\n",
      "\n",
      "         [[ 0.0944,  0.0623,  0.0685],\n",
      "          [-0.1211,  0.1232, -0.0852],\n",
      "          [ 0.0690, -0.0835, -0.0603]],\n",
      "\n",
      "         [[-0.0746,  0.0122, -0.1269],\n",
      "          [ 0.0503, -0.1332,  0.0911],\n",
      "          [ 0.1060, -0.0245,  0.0865]]],\n",
      "\n",
      "\n",
      "        [[[-0.0053, -0.1319,  0.0349],\n",
      "          [-0.0928,  0.0536,  0.1248],\n",
      "          [ 0.0056, -0.0551, -0.1047]],\n",
      "\n",
      "         [[-0.0146,  0.0974,  0.0151],\n",
      "          [-0.0377,  0.0691,  0.0277],\n",
      "          [ 0.0477, -0.0017, -0.0710]],\n",
      "\n",
      "         [[ 0.0582, -0.1119,  0.1073],\n",
      "          [-0.0269,  0.0527, -0.1034],\n",
      "          [ 0.1126,  0.0558, -0.1361]],\n",
      "\n",
      "         [[-0.0798,  0.0964, -0.0357],\n",
      "          [-0.0706,  0.0445, -0.0406],\n",
      "          [-0.0902,  0.1113, -0.0356]],\n",
      "\n",
      "         [[-0.0244, -0.0471, -0.0237],\n",
      "          [-0.1096, -0.0942,  0.1044],\n",
      "          [-0.1152, -0.0320, -0.0900]],\n",
      "\n",
      "         [[ 0.0812, -0.0161,  0.0872],\n",
      "          [-0.1141, -0.0896,  0.0771],\n",
      "          [ 0.0241, -0.1339, -0.0026]]],\n",
      "\n",
      "\n",
      "        [[[-0.0128,  0.1265,  0.1221],\n",
      "          [-0.0069, -0.1295, -0.1145],\n",
      "          [ 0.0083,  0.0608,  0.0224]],\n",
      "\n",
      "         [[ 0.1088, -0.0220,  0.0573],\n",
      "          [ 0.0225, -0.0294, -0.0465],\n",
      "          [-0.1062,  0.0550,  0.0485]],\n",
      "\n",
      "         [[ 0.0373, -0.0008,  0.0873],\n",
      "          [-0.1066,  0.0870,  0.1048],\n",
      "          [ 0.0416,  0.1036,  0.0825]],\n",
      "\n",
      "         [[-0.0172, -0.0193, -0.0750],\n",
      "          [ 0.0544, -0.0663, -0.0688],\n",
      "          [ 0.0620, -0.0108, -0.0474]],\n",
      "\n",
      "         [[-0.1310,  0.0722,  0.0289],\n",
      "          [-0.0297,  0.0649,  0.1103],\n",
      "          [ 0.0123, -0.0249, -0.0722]],\n",
      "\n",
      "         [[ 0.0355, -0.0124, -0.0994],\n",
      "          [-0.0553,  0.0935,  0.0705],\n",
      "          [ 0.0640,  0.1244, -0.0096]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0975, -0.0877,  0.0601],\n",
      "          [-0.0989,  0.1113,  0.0548],\n",
      "          [-0.0755, -0.0495, -0.0478]],\n",
      "\n",
      "         [[ 0.0560, -0.0162,  0.1344],\n",
      "          [-0.1305, -0.0488,  0.0343],\n",
      "          [ 0.0148, -0.0233,  0.0035]],\n",
      "\n",
      "         [[-0.0423, -0.1216, -0.0507],\n",
      "          [ 0.1039, -0.0206,  0.0419],\n",
      "          [ 0.0652, -0.0837, -0.1230]],\n",
      "\n",
      "         [[ 0.0380,  0.0080, -0.1156],\n",
      "          [ 0.0445,  0.0255,  0.0822],\n",
      "          [ 0.0835,  0.1285,  0.0121]],\n",
      "\n",
      "         [[-0.1051,  0.0808, -0.1216],\n",
      "          [-0.0242, -0.0279, -0.1256],\n",
      "          [-0.1141, -0.0087,  0.0977]],\n",
      "\n",
      "         [[-0.0662,  0.1150,  0.1356],\n",
      "          [-0.0764, -0.0155, -0.0317],\n",
      "          [-0.1051, -0.0331,  0.1175]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1321, -0.0180,  0.1159],\n",
      "          [-0.1360, -0.1049, -0.0418],\n",
      "          [ 0.0273, -0.1263,  0.0211]],\n",
      "\n",
      "         [[-0.0703,  0.0017, -0.0124],\n",
      "          [ 0.0257, -0.1126, -0.0193],\n",
      "          [-0.0793, -0.0229,  0.0180]],\n",
      "\n",
      "         [[-0.0369,  0.0136, -0.1218],\n",
      "          [-0.0819, -0.0494,  0.0001],\n",
      "          [-0.1276, -0.0890, -0.1088]],\n",
      "\n",
      "         [[ 0.0768, -0.1174,  0.0896],\n",
      "          [-0.0900, -0.0038,  0.0145],\n",
      "          [-0.0539, -0.1025, -0.0435]],\n",
      "\n",
      "         [[ 0.1069, -0.0018, -0.0392],\n",
      "          [-0.0761,  0.0128,  0.0073],\n",
      "          [-0.0102, -0.0042, -0.0884]],\n",
      "\n",
      "         [[-0.0118,  0.0586,  0.0225],\n",
      "          [ 0.1011,  0.0143, -0.1028],\n",
      "          [ 0.0946, -0.0951, -0.0373]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0342,  0.0713,  0.0872],\n",
      "          [ 0.1112, -0.0878, -0.0136],\n",
      "          [-0.0101, -0.1134,  0.1276]],\n",
      "\n",
      "         [[-0.0214, -0.0843, -0.0678],\n",
      "          [-0.0075, -0.0643, -0.0002],\n",
      "          [-0.0453,  0.1296, -0.0832]],\n",
      "\n",
      "         [[-0.1285,  0.0644, -0.0397],\n",
      "          [-0.1277,  0.1286,  0.1021],\n",
      "          [ 0.0748, -0.0592, -0.1021]],\n",
      "\n",
      "         [[ 0.0457, -0.0902, -0.1000],\n",
      "          [-0.0243, -0.0311,  0.0597],\n",
      "          [-0.0520,  0.0596,  0.0442]],\n",
      "\n",
      "         [[-0.0088, -0.0229, -0.0420],\n",
      "          [ 0.1351, -0.0068, -0.0579],\n",
      "          [-0.0936, -0.0811, -0.1252]],\n",
      "\n",
      "         [[ 0.0466,  0.0747,  0.0887],\n",
      "          [-0.1087,  0.1342, -0.1173],\n",
      "          [ 0.0010,  0.0888, -0.0638]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0241, -0.1094, -0.0494],\n",
      "          [ 0.0893,  0.0560,  0.0272],\n",
      "          [ 0.1071, -0.0288,  0.0925]],\n",
      "\n",
      "         [[ 0.0882,  0.1351, -0.0006],\n",
      "          [ 0.0656, -0.1358, -0.0690],\n",
      "          [-0.0959, -0.1201,  0.1184]],\n",
      "\n",
      "         [[-0.0211, -0.1093,  0.0346],\n",
      "          [-0.0659,  0.0450, -0.0495],\n",
      "          [-0.0794,  0.0283, -0.0083]],\n",
      "\n",
      "         [[ 0.0448, -0.0364, -0.0361],\n",
      "          [ 0.0401,  0.1331, -0.0113],\n",
      "          [ 0.0140,  0.0441,  0.0269]],\n",
      "\n",
      "         [[-0.1027,  0.1258,  0.1148],\n",
      "          [-0.0747,  0.0049, -0.0356],\n",
      "          [ 0.0238, -0.0681,  0.0534]],\n",
      "\n",
      "         [[ 0.0969,  0.0444, -0.0429],\n",
      "          [ 0.1330,  0.0040,  0.0355],\n",
      "          [ 0.0268,  0.0586,  0.0175]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1117, -0.0830, -0.0330],\n",
      "          [-0.1057, -0.1110, -0.1168],\n",
      "          [ 0.0540,  0.1241,  0.1313]],\n",
      "\n",
      "         [[ 0.0467, -0.1202, -0.0064],\n",
      "          [-0.1223,  0.1084, -0.0270],\n",
      "          [ 0.0648, -0.0055, -0.1258]],\n",
      "\n",
      "         [[ 0.0400,  0.1303,  0.1025],\n",
      "          [-0.0455,  0.0538,  0.0735],\n",
      "          [ 0.0279,  0.1355, -0.0453]],\n",
      "\n",
      "         [[-0.0584,  0.1251,  0.0310],\n",
      "          [-0.0181,  0.0585, -0.0531],\n",
      "          [-0.0154, -0.0320, -0.0643]],\n",
      "\n",
      "         [[-0.0942,  0.1203,  0.1008],\n",
      "          [-0.0274, -0.1334,  0.0463],\n",
      "          [ 0.0958, -0.0890, -0.0572]],\n",
      "\n",
      "         [[ 0.0630, -0.1078, -0.0391],\n",
      "          [-0.0799,  0.1292,  0.0402],\n",
      "          [ 0.0865,  0.0018,  0.0648]]],\n",
      "\n",
      "\n",
      "        [[[-0.1053, -0.0784,  0.1131],\n",
      "          [ 0.0792, -0.0135, -0.0500],\n",
      "          [ 0.0582,  0.0610, -0.0652]],\n",
      "\n",
      "         [[ 0.0163,  0.0590,  0.0851],\n",
      "          [ 0.0197, -0.0469, -0.1220],\n",
      "          [ 0.0992, -0.0802, -0.0166]],\n",
      "\n",
      "         [[ 0.0256,  0.1335, -0.1132],\n",
      "          [ 0.0499, -0.1304,  0.0047],\n",
      "          [ 0.0742,  0.0003, -0.0809]],\n",
      "\n",
      "         [[ 0.0708,  0.0461, -0.0628],\n",
      "          [-0.1267,  0.0240,  0.0982],\n",
      "          [ 0.1227, -0.0950,  0.0710]],\n",
      "\n",
      "         [[ 0.1171, -0.0968, -0.0764],\n",
      "          [-0.0872,  0.0057,  0.1067],\n",
      "          [-0.1219,  0.1052,  0.0794]],\n",
      "\n",
      "         [[-0.0645,  0.0029, -0.1204],\n",
      "          [ 0.1185,  0.0731, -0.0403],\n",
      "          [ 0.0179,  0.0902, -0.0332]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0221,  0.0452, -0.0932],\n",
      "          [ 0.1162,  0.0534, -0.0548],\n",
      "          [-0.0984, -0.0533,  0.1013]],\n",
      "\n",
      "         [[ 0.0120,  0.1065, -0.1059],\n",
      "          [-0.0531,  0.0178, -0.1007],\n",
      "          [ 0.0761, -0.0165,  0.0072]],\n",
      "\n",
      "         [[-0.1283,  0.0523,  0.0079],\n",
      "          [ 0.1244, -0.0784, -0.0402],\n",
      "          [ 0.0187, -0.0791, -0.1347]],\n",
      "\n",
      "         [[ 0.1066,  0.1291,  0.1353],\n",
      "          [ 0.0761, -0.0757, -0.0860],\n",
      "          [-0.1136, -0.1065,  0.0514]],\n",
      "\n",
      "         [[-0.1308, -0.0745,  0.1271],\n",
      "          [ 0.0811, -0.0143,  0.1101],\n",
      "          [-0.0865,  0.0395,  0.0663]],\n",
      "\n",
      "         [[ 0.0829, -0.1303, -0.0822],\n",
      "          [ 0.0016, -0.0508, -0.0977],\n",
      "          [-0.1194,  0.1084,  0.0572]]],\n",
      "\n",
      "\n",
      "        [[[-0.1159,  0.0947,  0.0437],\n",
      "          [ 0.0527, -0.1065,  0.1219],\n",
      "          [-0.1168,  0.0241, -0.0973]],\n",
      "\n",
      "         [[-0.0548,  0.0080,  0.0744],\n",
      "          [-0.1202, -0.0455, -0.0231],\n",
      "          [ 0.0070,  0.0135,  0.1003]],\n",
      "\n",
      "         [[ 0.0799,  0.1091,  0.0598],\n",
      "          [-0.0736,  0.0562, -0.1089],\n",
      "          [ 0.0397,  0.0907, -0.0381]],\n",
      "\n",
      "         [[-0.0190,  0.0118, -0.0525],\n",
      "          [-0.0630,  0.0882,  0.0972],\n",
      "          [-0.0765,  0.0768,  0.1189]],\n",
      "\n",
      "         [[ 0.0656, -0.0828, -0.1203],\n",
      "          [-0.1245, -0.0910, -0.0828],\n",
      "          [-0.0021,  0.0923, -0.1338]],\n",
      "\n",
      "         [[ 0.0041, -0.0311, -0.0838],\n",
      "          [ 0.0861,  0.0694, -0.0899],\n",
      "          [-0.0565, -0.0724, -0.0884]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0212, -0.0463, -0.0698],\n",
      "          [-0.0768,  0.0284,  0.0323],\n",
      "          [ 0.0794,  0.0676, -0.0601]],\n",
      "\n",
      "         [[-0.0404, -0.1211, -0.1218],\n",
      "          [ 0.1272,  0.1154,  0.0175],\n",
      "          [ 0.0768,  0.0454, -0.0322]],\n",
      "\n",
      "         [[ 0.0901, -0.0663, -0.0717],\n",
      "          [-0.0200,  0.0139,  0.0568],\n",
      "          [ 0.1193,  0.0572,  0.0301]],\n",
      "\n",
      "         [[-0.1289, -0.0877, -0.1306],\n",
      "          [-0.0160, -0.0439,  0.1300],\n",
      "          [-0.0013,  0.0125,  0.0927]],\n",
      "\n",
      "         [[-0.0924,  0.1351, -0.0616],\n",
      "          [ 0.0741,  0.1249,  0.0599],\n",
      "          [ 0.0898,  0.0863,  0.0581]],\n",
      "\n",
      "         [[ 0.0491,  0.0496, -0.1096],\n",
      "          [ 0.0540, -0.0542, -0.0476],\n",
      "          [-0.0347, -0.0841, -0.1191]]]], requires_grad=True)), ('fc1.bias', Parameter containing:\n",
      "tensor([ 0.0046, -0.0075, -0.0172,  0.0036,  0.0161, -0.0373, -0.0331, -0.0378,\n",
      "         0.0079,  0.0182, -0.0484, -0.0157, -0.0286, -0.0441,  0.0155,  0.0211,\n",
      "         0.0267,  0.0091,  0.0259, -0.0251,  0.0327, -0.0045, -0.0422, -0.0382,\n",
      "         0.0270, -0.0067,  0.0444, -0.0265,  0.0037, -0.0358, -0.0128,  0.0487,\n",
      "        -0.0314,  0.0034, -0.0282, -0.0123,  0.0338, -0.0463, -0.0191, -0.0111,\n",
      "        -0.0071, -0.0437, -0.0031, -0.0131,  0.0248,  0.0297, -0.0254,  0.0435,\n",
      "         0.0126, -0.0346,  0.0307,  0.0297,  0.0200,  0.0465, -0.0040, -0.0450,\n",
      "         0.0234,  0.0276,  0.0066,  0.0433,  0.0445, -0.0149,  0.0356, -0.0048,\n",
      "         0.0206, -0.0291, -0.0349, -0.0228, -0.0213,  0.0361, -0.0360,  0.0381,\n",
      "        -0.0456,  0.0277,  0.0275,  0.0361, -0.0396, -0.0072,  0.0057, -0.0462,\n",
      "         0.0091, -0.0024, -0.0448, -0.0388, -0.0332, -0.0220,  0.0072, -0.0104,\n",
      "         0.0343,  0.0026, -0.0495, -0.0036,  0.0007, -0.0314, -0.0007,  0.0413,\n",
      "        -0.0215, -0.0493,  0.0246,  0.0157,  0.0020, -0.0459,  0.0331, -0.0035,\n",
      "        -0.0239, -0.0082, -0.0426, -0.0247,  0.0298, -0.0385, -0.0163, -0.0382,\n",
      "         0.0320,  0.0078,  0.0162, -0.0402, -0.0125,  0.0189,  0.0056, -0.0009],\n",
      "       requires_grad=True)), ('fc1.weight_orig', Parameter containing:\n",
      "tensor([[ 0.0462, -0.0186, -0.0174,  ..., -0.0106,  0.0324, -0.0216],\n",
      "        [-0.0374,  0.0121,  0.0216,  ...,  0.0432, -0.0185,  0.0090],\n",
      "        [ 0.0485, -0.0183, -0.0275,  ..., -0.0223,  0.0335,  0.0146],\n",
      "        ...,\n",
      "        [ 0.0056, -0.0434, -0.0461,  ..., -0.0477, -0.0214, -0.0350],\n",
      "        [-0.0076, -0.0223, -0.0039,  ...,  0.0207, -0.0111, -0.0237],\n",
      "        [ 0.0307,  0.0411, -0.0320,  ...,  0.0470,  0.0088, -0.0062]],\n",
      "       requires_grad=True)), ('fc2.bias', Parameter containing:\n",
      "tensor([ 0.0263,  0.0665, -0.0008,  0.0675,  0.0740,  0.0624, -0.0878,  0.0744,\n",
      "         0.0669, -0.0871, -0.0088, -0.0445,  0.0867, -0.0701, -0.0893, -0.0098,\n",
      "        -0.0510,  0.0242,  0.0220, -0.0517, -0.0837, -0.0032,  0.0696,  0.0123,\n",
      "        -0.0469, -0.0358, -0.0057,  0.0285, -0.0112,  0.0676, -0.0240,  0.0095,\n",
      "         0.0400,  0.0705,  0.0783, -0.0442,  0.0429, -0.0095, -0.0853, -0.0186,\n",
      "        -0.0073, -0.0834, -0.0321, -0.0013, -0.0673,  0.0608,  0.0815, -0.0334,\n",
      "         0.0020, -0.0255,  0.0266,  0.0050, -0.0655,  0.0796, -0.0896, -0.0848,\n",
      "         0.0378, -0.0769,  0.0818, -0.0834,  0.0904,  0.0839, -0.0262,  0.0042,\n",
      "         0.0512,  0.0056,  0.0528,  0.0626,  0.0703,  0.0168, -0.0279,  0.0168,\n",
      "        -0.0350,  0.0334, -0.0350, -0.0657,  0.0259,  0.0554, -0.0056, -0.0527,\n",
      "         0.0820,  0.0202, -0.0557, -0.0002], requires_grad=True)), ('fc2.weight_orig', Parameter containing:\n",
      "tensor([[ 0.0318, -0.0633,  0.0208,  ...,  0.0716, -0.0595,  0.0457],\n",
      "        [-0.0441, -0.0383,  0.0426,  ..., -0.0047,  0.0413,  0.0651],\n",
      "        [ 0.0546, -0.0592, -0.0064,  ...,  0.0702, -0.0072,  0.0733],\n",
      "        ...,\n",
      "        [-0.0746,  0.0352,  0.0705,  ...,  0.0197,  0.0266, -0.0157],\n",
      "        [-0.0547, -0.0245,  0.0590,  ..., -0.0855, -0.0870, -0.0463],\n",
      "        [-0.0651, -0.0902,  0.0348,  ...,  0.0630, -0.0553,  0.0891]],\n",
      "       requires_grad=True)), ('fc3.bias', Parameter containing:\n",
      "tensor([-0.1040,  0.0748, -0.0630, -0.0142,  0.0443, -0.0804,  0.0838,  0.0271,\n",
      "         0.0849, -0.0600], requires_grad=True)), ('fc3.weight_orig', Parameter containing:\n",
      "tensor([[ 0.0729, -0.0372,  0.0078,  0.0935,  0.0697,  0.0264, -0.0430, -0.0664,\n",
      "          0.0746,  0.1044,  0.0370,  0.0902, -0.0847,  0.0589,  0.0095, -0.0809,\n",
      "          0.0772, -0.0803,  0.0183,  0.0280,  0.0775,  0.0837, -0.0150,  0.0831,\n",
      "          0.0094, -0.0318, -0.0938,  0.0509,  0.0370,  0.0611, -0.0020,  0.0441,\n",
      "          0.0819,  0.0589,  0.0186, -0.1041,  0.0281, -0.0839,  0.0093,  0.0812,\n",
      "          0.0924, -0.0199,  0.0477,  0.0282,  0.0035, -0.0833,  0.0238, -0.0734,\n",
      "         -0.0029,  0.0973,  0.0900,  0.0984,  0.1073, -0.0340, -0.0550, -0.0801,\n",
      "         -0.0772, -0.0873,  0.0636, -0.0220, -0.0815, -0.0562,  0.0558,  0.0118,\n",
      "          0.0417,  0.0254, -0.0298,  0.0496, -0.0032,  0.0834,  0.0271, -0.0736,\n",
      "         -0.0126, -0.0958,  0.0494,  0.0439,  0.0210,  0.0740, -0.0467,  0.0416,\n",
      "          0.0474,  0.0522,  0.0624, -0.0865],\n",
      "        [-0.0416,  0.0631, -0.0459, -0.0078, -0.0693,  0.0051,  0.0706,  0.0992,\n",
      "         -0.0545,  0.0320,  0.0333,  0.0029, -0.0812, -0.0738, -0.0988, -0.0299,\n",
      "         -0.0888, -0.1084, -0.0291,  0.0343,  0.0376,  0.0302,  0.0715, -0.0848,\n",
      "          0.0579,  0.0014, -0.1052,  0.0680, -0.0267, -0.0612, -0.0732, -0.0574,\n",
      "         -0.0669,  0.0203,  0.0808, -0.1050,  0.0599, -0.0678, -0.0451, -0.0202,\n",
      "          0.0431, -0.0009, -0.0383,  0.1010, -0.0959, -0.0419, -0.0624,  0.1039,\n",
      "         -0.0738,  0.0090,  0.1068, -0.0577,  0.0923,  0.0885,  0.0529, -0.0154,\n",
      "         -0.0615,  0.0025, -0.0380,  0.0573,  0.0764, -0.0175, -0.0224, -0.0264,\n",
      "          0.0680, -0.0913,  0.0049,  0.0784, -0.0478, -0.0829,  0.0445,  0.1009,\n",
      "          0.0668, -0.0358, -0.0081, -0.0361, -0.0920, -0.0232, -0.0111, -0.0638,\n",
      "         -0.0817, -0.0378,  0.0131,  0.0730],\n",
      "        [-0.0066, -0.0249, -0.0488,  0.1030,  0.0994, -0.0819, -0.0742,  0.0500,\n",
      "          0.0731,  0.0592,  0.1013,  0.0745, -0.0784, -0.0305, -0.0883,  0.0874,\n",
      "         -0.0066,  0.0007, -0.0126, -0.0704, -0.0220, -0.0034,  0.0581,  0.0201,\n",
      "         -0.0545, -0.1009, -0.0528,  0.0535, -0.0299,  0.0636,  0.0151, -0.0634,\n",
      "         -0.0946,  0.0532, -0.0372,  0.0734,  0.0624,  0.0550, -0.0425,  0.0944,\n",
      "          0.0356,  0.0890,  0.0382,  0.0575,  0.0406, -0.0653,  0.0015,  0.1059,\n",
      "          0.0723,  0.0293,  0.0960, -0.0812, -0.0818, -0.0610, -0.0217,  0.1077,\n",
      "          0.0302, -0.0365,  0.0589,  0.0981,  0.0426, -0.0973,  0.0199,  0.0113,\n",
      "         -0.0021, -0.0735,  0.0947,  0.0763, -0.0081, -0.0018, -0.0476, -0.0411,\n",
      "         -0.0105,  0.0512,  0.0389, -0.1060,  0.0827,  0.0241, -0.0768, -0.0420,\n",
      "          0.0379, -0.0528,  0.0599,  0.0952],\n",
      "        [-0.0243,  0.0810,  0.0911,  0.0958, -0.0814, -0.0651, -0.0018, -0.0415,\n",
      "          0.0209,  0.0138,  0.0074, -0.0299, -0.0264, -0.0835,  0.1009,  0.0716,\n",
      "          0.0146,  0.0344,  0.0500, -0.1043,  0.0012, -0.0382,  0.0769, -0.0709,\n",
      "         -0.0149,  0.0882, -0.0903,  0.0222,  0.0512,  0.0348,  0.0214,  0.0245,\n",
      "         -0.0720,  0.0173,  0.0875,  0.0886,  0.0722, -0.0863,  0.0452,  0.0049,\n",
      "         -0.0479,  0.0637, -0.1057, -0.0624,  0.0612, -0.0211, -0.0417, -0.1005,\n",
      "          0.0271, -0.0576, -0.0597, -0.0002,  0.0674,  0.0206, -0.0838, -0.0505,\n",
      "         -0.0318, -0.0540,  0.0924,  0.0490,  0.1012, -0.0457, -0.1025,  0.0413,\n",
      "          0.0081,  0.0114,  0.0324,  0.0026,  0.0787, -0.0752,  0.0985, -0.0129,\n",
      "         -0.0512, -0.0363, -0.0510, -0.0887,  0.0245, -0.0637,  0.0617,  0.1052,\n",
      "          0.0659,  0.0187, -0.0601, -0.0986],\n",
      "        [ 0.0512, -0.1074, -0.0711,  0.0707, -0.0838,  0.1012, -0.0587,  0.0781,\n",
      "          0.0706, -0.0011, -0.0188, -0.0212, -0.0196,  0.0090,  0.1057, -0.0685,\n",
      "         -0.0559,  0.0359,  0.0989,  0.0552, -0.0213, -0.1032, -0.1035, -0.0028,\n",
      "         -0.0450, -0.0285, -0.0031, -0.0717, -0.0419,  0.0425,  0.0806, -0.0723,\n",
      "         -0.0888,  0.0334, -0.0708, -0.0745, -0.0788,  0.0475, -0.0164,  0.0351,\n",
      "          0.0316,  0.0592, -0.0965,  0.0113, -0.0241, -0.0798, -0.0933, -0.1047,\n",
      "         -0.0506, -0.1019, -0.0348,  0.0852,  0.0890, -0.0482,  0.1067, -0.0412,\n",
      "         -0.0500, -0.0827,  0.0406, -0.0889, -0.0430, -0.1022,  0.0837,  0.0958,\n",
      "         -0.1028, -0.0425,  0.0797, -0.0271,  0.0087,  0.0093, -0.0984,  0.0602,\n",
      "         -0.0565, -0.0431, -0.0235, -0.0158,  0.1085, -0.0904,  0.0952,  0.0025,\n",
      "          0.0857, -0.0407, -0.0686,  0.0067],\n",
      "        [-0.0878,  0.1028,  0.0921,  0.0537, -0.0069,  0.0693,  0.1065,  0.0357,\n",
      "          0.1017, -0.0287, -0.0560, -0.0833,  0.0730,  0.0458, -0.0045, -0.0845,\n",
      "         -0.0628,  0.0791,  0.0943,  0.0293, -0.0884, -0.0909, -0.0736,  0.0020,\n",
      "          0.0144,  0.0837, -0.0204,  0.0348, -0.0268,  0.0425,  0.0017,  0.0742,\n",
      "          0.0030,  0.0695, -0.0713, -0.1048,  0.0927, -0.0599,  0.0317, -0.0080,\n",
      "          0.0632,  0.0725,  0.0225,  0.0977, -0.0806,  0.1083,  0.0975, -0.0757,\n",
      "         -0.0846,  0.0303,  0.0828, -0.1007, -0.0578,  0.0314,  0.0449, -0.0849,\n",
      "         -0.0263, -0.0182,  0.0841, -0.0815, -0.1087,  0.0648,  0.0461,  0.0366,\n",
      "         -0.0914,  0.0471, -0.0667, -0.0020,  0.0627,  0.0664,  0.0774,  0.0769,\n",
      "         -0.0479,  0.0067, -0.0652, -0.0646,  0.0847,  0.0055,  0.0855, -0.0788,\n",
      "         -0.0774,  0.0792, -0.0941, -0.0393],\n",
      "        [ 0.0268,  0.0429, -0.0204,  0.0312, -0.1060,  0.0810, -0.0106, -0.0883,\n",
      "          0.0663,  0.0038, -0.0005, -0.0842,  0.0889,  0.0192,  0.0366, -0.0890,\n",
      "          0.0385,  0.1069, -0.0665, -0.0582,  0.0570, -0.0757, -0.1038, -0.0331,\n",
      "          0.0121,  0.0771, -0.0639,  0.0699, -0.0034, -0.0051,  0.1008,  0.0056,\n",
      "          0.0980, -0.0251,  0.0941, -0.0336, -0.0262, -0.0635, -0.0417,  0.0225,\n",
      "          0.0439, -0.0091, -0.0248, -0.0400,  0.1031, -0.0118,  0.0038, -0.0437,\n",
      "          0.0490, -0.0730, -0.0693,  0.1062, -0.0061, -0.0894, -0.0231, -0.0335,\n",
      "          0.0093, -0.0170,  0.1020,  0.0558,  0.0720,  0.0785, -0.0531,  0.0206,\n",
      "          0.0886, -0.0673, -0.0321, -0.0086,  0.1071,  0.0235, -0.0750, -0.0256,\n",
      "          0.0031,  0.0710,  0.0064,  0.0885,  0.0497,  0.0999,  0.0969,  0.0716,\n",
      "         -0.0228,  0.0782, -0.0432, -0.0013],\n",
      "        [ 0.0860,  0.0388, -0.0598,  0.1010, -0.0138, -0.0325,  0.0800, -0.0040,\n",
      "          0.0672, -0.0767,  0.0601, -0.0797, -0.0142,  0.0012,  0.0041,  0.0722,\n",
      "          0.0879, -0.0910,  0.0083, -0.0271,  0.0605,  0.0572,  0.0223, -0.0508,\n",
      "         -0.0474,  0.0905,  0.0524,  0.0312, -0.1019,  0.0741,  0.0801,  0.0675,\n",
      "          0.0152,  0.0286, -0.0920,  0.0556,  0.0560,  0.1065,  0.0110, -0.1088,\n",
      "         -0.0921,  0.0394,  0.0857,  0.0562, -0.0623,  0.0722, -0.0095, -0.0058,\n",
      "         -0.0994, -0.0309, -0.0873,  0.0227, -0.0583, -0.1036, -0.0096,  0.0030,\n",
      "         -0.0602, -0.0957,  0.0735,  0.0894,  0.0169, -0.0346, -0.0386, -0.0180,\n",
      "          0.1071,  0.0184,  0.0934, -0.0229, -0.0072,  0.1056, -0.0729,  0.0178,\n",
      "         -0.0715,  0.0132,  0.0257, -0.0082, -0.0665, -0.0311,  0.1048,  0.0417,\n",
      "         -0.0271,  0.0720,  0.0309, -0.0030],\n",
      "        [ 0.0964,  0.0051,  0.0201,  0.1007,  0.0073,  0.0116,  0.0041, -0.0572,\n",
      "         -0.0127, -0.0695,  0.0116,  0.0347,  0.0526,  0.0655,  0.0865,  0.0642,\n",
      "         -0.0853,  0.0359, -0.0453, -0.0951,  0.0720, -0.0795,  0.0174, -0.0317,\n",
      "         -0.0228,  0.0421, -0.0903,  0.0041,  0.0919, -0.0833,  0.0481, -0.0952,\n",
      "         -0.1052,  0.0427, -0.0237,  0.0977,  0.0480,  0.0242, -0.0489,  0.0032,\n",
      "          0.0167,  0.0153, -0.0006, -0.0013,  0.0065, -0.0089, -0.0619, -0.0435,\n",
      "         -0.0779,  0.0958,  0.0917, -0.0881, -0.0215, -0.0762, -0.0260,  0.0222,\n",
      "          0.0521, -0.0802,  0.0751,  0.0596, -0.0304,  0.0899,  0.0779,  0.0188,\n",
      "         -0.0245,  0.0659, -0.0968,  0.0345,  0.0946,  0.0833, -0.0072,  0.0643,\n",
      "         -0.0869,  0.0105, -0.0140, -0.0887,  0.0277, -0.0320,  0.0145, -0.0596,\n",
      "          0.0163,  0.0378,  0.0278,  0.0519],\n",
      "        [-0.0869,  0.0601,  0.1055,  0.0873,  0.1059,  0.0819, -0.0010, -0.0650,\n",
      "         -0.0709,  0.0246,  0.0954,  0.0639,  0.0981, -0.0892, -0.0974, -0.0053,\n",
      "          0.0038,  0.0060,  0.0057,  0.0545, -0.0953, -0.0976, -0.0430, -0.0444,\n",
      "          0.0008, -0.1014, -0.1050, -0.0375, -0.0910,  0.0428, -0.0339, -0.1027,\n",
      "         -0.0754,  0.0464, -0.0046, -0.0071,  0.0212,  0.1003, -0.0834,  0.0434,\n",
      "         -0.0524,  0.0605,  0.0894, -0.0105,  0.0587,  0.0604, -0.0680, -0.0833,\n",
      "         -0.0947, -0.0551, -0.0514, -0.0335,  0.0428, -0.0473,  0.0121, -0.0133,\n",
      "         -0.0641,  0.0529,  0.1029,  0.0457,  0.0023,  0.0826, -0.0875,  0.0355,\n",
      "          0.0858,  0.0040,  0.0291, -0.0156, -0.0139, -0.0407, -0.1068, -0.0786,\n",
      "          0.0377, -0.0469,  0.0829,  0.1034,  0.0830,  0.0791, -0.0827, -0.0313,\n",
      "         -0.0639,  0.0404,  0.0238,  0.0833]], requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "print(list(new_model.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Parameter 'weight' of module LeNet(\n  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n  (fc1): Linear(in_features=400, out_features=120, bias=True)\n  (fc2): Linear(in_features=120, out_features=84, bias=True)\n  (fc3): Linear(in_features=84, out_features=10, bias=True)\n) has to be pruned before pruning can be removed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-d99d217999a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprune\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'weight'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/utils/prune.py\u001b[0m in \u001b[0;36mremove\u001b[0;34m(module, name)\u001b[0m\n\u001b[1;32m   1122\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m     raise ValueError(\n\u001b[0m\u001b[1;32m   1125\u001b[0m         \u001b[0;34m\"Parameter '{}' of module {} has to be pruned \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m         \u001b[0;34m\"before pruning can be removed\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Parameter 'weight' of module LeNet(\n  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n  (fc1): Linear(in_features=400, out_features=120, bias=True)\n  (fc2): Linear(in_features=120, out_features=84, bias=True)\n  (fc3): Linear(in_features=84, out_features=10, bias=True)\n) has to be pruned before pruning can be removed"
     ]
    }
   ],
   "source": [
    "prune.remove(new_model, 'weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Linear(in_features=400, out_features=120, bias=True)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Linear(in_features=120, out_features=84, bias=True)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Linear(in_features=84, out_features=10, bias=True)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for name, module in new_model.named_modules():\n",
    "#     print(name)\n",
    "#     print(hasattr(module, \"weight\"))\n",
    "    if hasattr(module, \"weight\"):\n",
    "        prune.remove(module, 'weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "621 µs ± 65.6 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "b = new_model(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LeNet(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n",
      "conv1\n",
      "Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "conv2\n",
      "Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "fc1\n",
      "Linear(in_features=400, out_features=120, bias=True)\n",
      "fc2\n",
      "Linear(in_features=120, out_features=84, bias=True)\n",
      "fc3\n",
      "Linear(in_features=84, out_features=10, bias=True)\n"
     ]
    }
   ],
   "source": [
    "for name, module in new_model.named_modules():\n",
    "    print(name)\n",
    "    print(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('conv1.bias', Parameter containing:\n",
      "tensor([-0.0548,  0.0290, -0.0349,  0.0005,  0.2851,  0.1343],\n",
      "       requires_grad=True)), ('conv1.weight', Parameter containing:\n",
      "tensor([[[[ 0.0000,  0.0000, -0.0000],\n",
      "          [-0.2591,  0.0000, -0.0000],\n",
      "          [-0.1820, -0.1879,  0.3151]]],\n",
      "\n",
      "\n",
      "        [[[-0.1805,  0.2011,  0.2644],\n",
      "          [-0.0000,  0.0000, -0.0000],\n",
      "          [-0.2450,  0.0000,  0.2964]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000, -0.3274,  0.0000],\n",
      "          [ 0.3038, -0.3070, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2814,  0.0000, -0.2850],\n",
      "          [-0.0000, -0.0000, -0.2134],\n",
      "          [-0.3121,  0.0000, -0.2524]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000,  0.0000,  0.2947],\n",
      "          [-0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.2175,  0.2236, -0.0000],\n",
      "          [-0.1700, -0.0000, -0.1669],\n",
      "          [ 0.0000, -0.0000, -0.0000]]]], requires_grad=True)), ('conv2.bias', Parameter containing:\n",
      "tensor([ 0.1171,  0.0828,  0.0715,  0.0084, -0.0181, -0.0844, -0.0797,  0.1087,\n",
      "         0.0421, -0.0049,  0.1055, -0.0737,  0.1003, -0.1129, -0.1161, -0.0387],\n",
      "       requires_grad=True)), ('conv2.weight', Parameter containing:\n",
      "tensor([[[[ 0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0892, -0.1297],\n",
      "          [ 0.0000, -0.0958, -0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0956,  0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "         [[-0.1132, -0.0000, -0.0000],\n",
      "          [-0.1270,  0.0000,  0.0870],\n",
      "          [ 0.0000, -0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.1260, -0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000],\n",
      "          [ 0.1076, -0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.1033, -0.0000, -0.1240],\n",
      "          [-0.0879,  0.0950,  0.0913],\n",
      "          [ 0.0979, -0.0000, -0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000, -0.0000],\n",
      "          [-0.0945,  0.0000,  0.1358],\n",
      "          [ 0.0000, -0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1341,  0.0853, -0.0000],\n",
      "          [-0.1292, -0.0000,  0.1278],\n",
      "          [-0.1317,  0.0000,  0.1109]],\n",
      "\n",
      "         [[-0.0000, -0.0000, -0.1177],\n",
      "          [ 0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.1247, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.1272],\n",
      "          [-0.0000, -0.1320,  0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0000,  0.0000],\n",
      "          [-0.0964,  0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000,  0.1089]],\n",
      "\n",
      "         [[ 0.0000, -0.0000, -0.0000],\n",
      "          [ 0.1360,  0.0000, -0.1146],\n",
      "          [-0.0000, -0.0000, -0.0897]],\n",
      "\n",
      "         [[ 0.0000, -0.1009,  0.1291],\n",
      "          [ 0.0828,  0.0000,  0.0000],\n",
      "          [-0.1102,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000, -0.0866,  0.0884],\n",
      "          [-0.1243,  0.0000, -0.1285],\n",
      "          [ 0.0000,  0.0000,  0.1143]],\n",
      "\n",
      "         [[-0.1139,  0.0000,  0.0926],\n",
      "          [ 0.1237,  0.0000,  0.1338],\n",
      "          [-0.0000, -0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [-0.0000, -0.0000,  0.1016],\n",
      "          [ 0.0000, -0.1293, -0.0000]],\n",
      "\n",
      "         [[ 0.1071, -0.0000, -0.0892],\n",
      "          [-0.1134, -0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0000,  0.1044, -0.0851],\n",
      "          [ 0.1284,  0.0000, -0.1014],\n",
      "          [-0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "         [[ 0.1091,  0.0000, -0.1293],\n",
      "          [ 0.0894, -0.0971,  0.0000],\n",
      "          [ 0.0000,  0.1223, -0.1313]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1256, -0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0904, -0.1354, -0.1165]],\n",
      "\n",
      "         [[-0.1071,  0.0000,  0.0000],\n",
      "          [-0.1337, -0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000,  0.0992]],\n",
      "\n",
      "         [[ 0.1347, -0.0000,  0.1072],\n",
      "          [-0.0000,  0.1071,  0.0000],\n",
      "          [ 0.0876, -0.0909, -0.0000]],\n",
      "\n",
      "         [[ 0.1296, -0.1290, -0.0000],\n",
      "          [ 0.0000,  0.1340,  0.1047],\n",
      "          [-0.0984,  0.1247,  0.1272]],\n",
      "\n",
      "         [[ 0.1023, -0.0000, -0.1114],\n",
      "          [ 0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.1259, -0.0828,  0.0000],\n",
      "          [-0.1101,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0925,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000,  0.0000,  0.1077],\n",
      "          [-0.0000, -0.1230, -0.0000],\n",
      "          [-0.1030,  0.1164, -0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0909,  0.0000],\n",
      "          [ 0.0000,  0.0000, -0.1157],\n",
      "          [ 0.0000, -0.1049, -0.0000]],\n",
      "\n",
      "         [[ 0.0000, -0.0898, -0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0817, -0.0000, -0.0975],\n",
      "          [-0.0875,  0.0000,  0.1189]],\n",
      "\n",
      "         [[-0.0000,  0.1219, -0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000,  0.1207]],\n",
      "\n",
      "         [[-0.0000, -0.1266,  0.0000],\n",
      "          [-0.1071,  0.0000,  0.1073],\n",
      "          [-0.0000, -0.1127, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.1292],\n",
      "          [ 0.1339, -0.0000, -0.0000],\n",
      "          [ 0.1029, -0.0000, -0.1015]],\n",
      "\n",
      "         [[ 0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0000,  0.1344,  0.0000],\n",
      "          [ 0.1267,  0.0000, -0.1273]],\n",
      "\n",
      "         [[ 0.0000, -0.1172, -0.1067],\n",
      "          [-0.1110,  0.0000, -0.0000],\n",
      "          [-0.1291, -0.0000, -0.0000]],\n",
      "\n",
      "         [[-0.0885,  0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0934,  0.0968],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.1020, -0.0000, -0.1160],\n",
      "          [ 0.0000,  0.1157,  0.0000],\n",
      "          [-0.0000, -0.1226,  0.1003]],\n",
      "\n",
      "         [[-0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000,  0.1351],\n",
      "          [ 0.0000,  0.1349, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0815, -0.0862, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0958, -0.0000],\n",
      "          [ 0.0000,  0.0965,  0.0964],\n",
      "          [-0.1151, -0.0000, -0.1140]],\n",
      "\n",
      "         [[-0.1250,  0.0867, -0.0000],\n",
      "          [-0.0000,  0.1112,  0.0000],\n",
      "          [-0.1121,  0.0000, -0.0000]],\n",
      "\n",
      "         [[ 0.1030, -0.0000,  0.0000],\n",
      "          [-0.1119, -0.0000, -0.0000],\n",
      "          [ 0.0992, -0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0851,  0.1294, -0.1021],\n",
      "          [ 0.0000, -0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0000,  0.1203],\n",
      "          [ 0.0000, -0.1115, -0.0000],\n",
      "          [ 0.1174,  0.0000,  0.1125]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000, -0.1043],\n",
      "          [ 0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0000, -0.1124],\n",
      "          [-0.0000,  0.0000, -0.1234],\n",
      "          [ 0.1186,  0.1112,  0.0000]],\n",
      "\n",
      "         [[-0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000,  0.1182],\n",
      "          [-0.0000, -0.0886,  0.0000]],\n",
      "\n",
      "         [[-0.0000,  0.0909,  0.0000],\n",
      "          [-0.0000, -0.0000, -0.1128],\n",
      "          [-0.0936, -0.0000,  0.1339]],\n",
      "\n",
      "         [[ 0.0930,  0.1176,  0.0000],\n",
      "          [ 0.0000,  0.1036, -0.0880],\n",
      "          [ 0.0000, -0.0000, -0.0887]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000,  0.0000,  0.1300],\n",
      "          [ 0.0000,  0.0000, -0.1262],\n",
      "          [ 0.0000, -0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.1068, -0.1066,  0.0000],\n",
      "          [-0.1196, -0.0911,  0.0000],\n",
      "          [-0.0000,  0.0853,  0.0000]],\n",
      "\n",
      "         [[ 0.1358,  0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0876,  0.1231],\n",
      "          [ 0.0000,  0.0000, -0.0000]],\n",
      "\n",
      "         [[-0.1016, -0.0000,  0.1228],\n",
      "          [ 0.0882, -0.0927,  0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000, -0.0922],\n",
      "          [ 0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000, -0.1135]],\n",
      "\n",
      "         [[-0.1307,  0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000,  0.0898],\n",
      "          [-0.1315, -0.1181,  0.0837]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1290,  0.1341, -0.1171],\n",
      "          [-0.1275,  0.0820, -0.0989],\n",
      "          [-0.0920,  0.0818,  0.0000]],\n",
      "\n",
      "         [[ 0.1203, -0.0905,  0.0990],\n",
      "          [-0.0000, -0.0000,  0.0941],\n",
      "          [ 0.1126, -0.0000, -0.0000]],\n",
      "\n",
      "         [[-0.1343, -0.0000,  0.1019],\n",
      "          [ 0.0000, -0.1146,  0.0000],\n",
      "          [-0.0000, -0.1248, -0.1299]],\n",
      "\n",
      "         [[-0.0000, -0.0000,  0.0897],\n",
      "          [-0.0920,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.1054, -0.1074]],\n",
      "\n",
      "         [[ 0.0848,  0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000, -0.1126, -0.1232]],\n",
      "\n",
      "         [[ 0.0000, -0.0000, -0.0000],\n",
      "          [-0.1233,  0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1198,  0.1099,  0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0840,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.1151,  0.0000,  0.0000],\n",
      "          [-0.0000,  0.1146,  0.1354],\n",
      "          [-0.0914,  0.0000, -0.0000]],\n",
      "\n",
      "         [[ 0.0000, -0.0000,  0.1075],\n",
      "          [ 0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.1279, -0.1331],\n",
      "          [-0.0000, -0.1197,  0.1003],\n",
      "          [ 0.0000,  0.0000,  0.1021]],\n",
      "\n",
      "         [[-0.0838,  0.1024, -0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000, -0.1229]],\n",
      "\n",
      "         [[ 0.0000, -0.0932,  0.1250],\n",
      "          [-0.0930, -0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000, -0.0000, -0.1080],\n",
      "          [-0.0822, -0.0000,  0.0000],\n",
      "          [ 0.0822, -0.0000,  0.0887]],\n",
      "\n",
      "         [[ 0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0897,  0.0920]],\n",
      "\n",
      "         [[-0.0000,  0.0814,  0.0940],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0824, -0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0818, -0.0000],\n",
      "          [ 0.0000, -0.1200,  0.0882],\n",
      "          [ 0.1245,  0.0000,  0.0839]],\n",
      "\n",
      "         [[ 0.0000, -0.0000, -0.0000],\n",
      "          [ 0.1247,  0.1289,  0.0000],\n",
      "          [-0.0000,  0.0000, -0.0997]],\n",
      "\n",
      "         [[ 0.1339, -0.0000,  0.1039],\n",
      "          [ 0.0890, -0.1038, -0.1013],\n",
      "          [ 0.0000, -0.1187,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.1254, -0.0826, -0.0000],\n",
      "          [-0.1335,  0.1216,  0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0909, -0.0000],\n",
      "          [-0.0000,  0.1251, -0.0827],\n",
      "          [ 0.0000,  0.0000, -0.0886]],\n",
      "\n",
      "         [[-0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0867, -0.0924],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.1138,  0.0000, -0.0000],\n",
      "          [-0.0852,  0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0000,  0.1348],\n",
      "          [-0.0000, -0.1256,  0.0000],\n",
      "          [ 0.1047,  0.1170,  0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0000,  0.1095],\n",
      "          [-0.1019, -0.0834,  0.0821],\n",
      "          [ 0.1330, -0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.1161,  0.0000],\n",
      "          [ 0.1157,  0.0000,  0.1025],\n",
      "          [ 0.0000, -0.0000,  0.0976]],\n",
      "\n",
      "         [[ 0.1243,  0.1078,  0.1354],\n",
      "          [ 0.1167,  0.0855,  0.0000],\n",
      "          [ 0.1293,  0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0861, -0.1138,  0.0000],\n",
      "          [ 0.0000,  0.0842, -0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000, -0.0000,  0.0000],\n",
      "          [ 0.1074, -0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000]],\n",
      "\n",
      "         [[ 0.1040, -0.0816, -0.1250],\n",
      "          [-0.1037,  0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000]],\n",
      "\n",
      "         [[-0.1190,  0.0968,  0.0923],\n",
      "          [-0.1122, -0.0000, -0.1278],\n",
      "          [-0.1096,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.1281, -0.0944,  0.0000],\n",
      "          [ 0.0000,  0.1357,  0.0000],\n",
      "          [-0.0827, -0.0000,  0.1114]],\n",
      "\n",
      "         [[ 0.0833,  0.1050,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0951],\n",
      "          [-0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0875, -0.1314,  0.1265],\n",
      "          [-0.0000,  0.1052, -0.0000]],\n",
      "\n",
      "         [[-0.1041,  0.1019, -0.0870],\n",
      "          [ 0.0000, -0.0000, -0.1149],\n",
      "          [-0.0000,  0.0000, -0.0000]],\n",
      "\n",
      "         [[ 0.0000, -0.1353,  0.1069],\n",
      "          [-0.0000, -0.0000, -0.0000],\n",
      "          [-0.1026,  0.0000, -0.0902]],\n",
      "\n",
      "         [[ 0.1133, -0.0000,  0.0000],\n",
      "          [ 0.0998,  0.1106,  0.0865],\n",
      "          [-0.1061,  0.1307, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.1218],\n",
      "          [ 0.0888,  0.0000, -0.0000]],\n",
      "\n",
      "         [[ 0.1151, -0.1057,  0.0000],\n",
      "          [ 0.1212,  0.1335, -0.1099],\n",
      "          [-0.0000, -0.0821,  0.0000]],\n",
      "\n",
      "         [[ 0.0000, -0.1278, -0.0000],\n",
      "          [ 0.0938, -0.1219, -0.1106],\n",
      "          [ 0.0000,  0.1258, -0.1164]],\n",
      "\n",
      "         [[ 0.1350,  0.0000, -0.0000],\n",
      "          [ 0.0944, -0.0000,  0.0000],\n",
      "          [ 0.0000, -0.1098, -0.1270]],\n",
      "\n",
      "         [[-0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000,  0.1306, -0.0000],\n",
      "          [ 0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "         [[-0.1106, -0.1064,  0.1120],\n",
      "          [-0.0981, -0.0929,  0.0000],\n",
      "          [ 0.0000, -0.0000, -0.1236]]]], requires_grad=True)), ('fc1.bias', Parameter containing:\n",
      "tensor([ 0.0199, -0.0254, -0.0320, -0.0030, -0.0482,  0.0278, -0.0451, -0.0088,\n",
      "        -0.0130,  0.0170,  0.0343,  0.0092, -0.0028, -0.0272,  0.0442, -0.0445,\n",
      "         0.0008,  0.0470, -0.0105,  0.0457, -0.0280,  0.0232, -0.0287, -0.0278,\n",
      "         0.0178, -0.0247,  0.0495,  0.0025, -0.0472,  0.0152, -0.0321, -0.0424,\n",
      "         0.0302, -0.0136,  0.0450, -0.0205,  0.0343, -0.0225,  0.0375, -0.0134,\n",
      "         0.0283, -0.0083, -0.0459, -0.0192,  0.0312,  0.0359,  0.0200, -0.0498,\n",
      "        -0.0019,  0.0445, -0.0495, -0.0331, -0.0055,  0.0086,  0.0338, -0.0306,\n",
      "         0.0041,  0.0221, -0.0056, -0.0463,  0.0278,  0.0209,  0.0439, -0.0160,\n",
      "         0.0314, -0.0203, -0.0340, -0.0210,  0.0331, -0.0029,  0.0114,  0.0348,\n",
      "         0.0313, -0.0025,  0.0271, -0.0250, -0.0034,  0.0303, -0.0101, -0.0286,\n",
      "        -0.0076,  0.0419, -0.0148,  0.0434,  0.0251,  0.0188,  0.0089, -0.0311,\n",
      "         0.0145,  0.0365, -0.0350, -0.0240,  0.0363, -0.0130, -0.0071, -0.0150,\n",
      "        -0.0455,  0.0005,  0.0314, -0.0063, -0.0254,  0.0251,  0.0041,  0.0089,\n",
      "        -0.0382, -0.0036,  0.0114,  0.0354, -0.0322,  0.0297,  0.0133,  0.0489,\n",
      "        -0.0091,  0.0167, -0.0370,  0.0137,  0.0178, -0.0279,  0.0207, -0.0072],\n",
      "       requires_grad=True)), ('fc1.weight', Parameter containing:\n",
      "tensor([[ 0.0292,  0.0243,  0.0000,  ..., -0.0467, -0.0242, -0.0418],\n",
      "        [ 0.0000,  0.0282,  0.0373,  ...,  0.0404, -0.0234,  0.0427],\n",
      "        [-0.0413,  0.0292,  0.0000,  ..., -0.0255,  0.0298,  0.0000],\n",
      "        ...,\n",
      "        [-0.0288,  0.0225, -0.0000,  ..., -0.0391, -0.0000,  0.0465],\n",
      "        [-0.0000,  0.0000,  0.0252,  ...,  0.0427,  0.0313, -0.0218],\n",
      "        [-0.0458, -0.0356,  0.0436,  ...,  0.0000, -0.0313,  0.0450]],\n",
      "       requires_grad=True)), ('fc2.bias', Parameter containing:\n",
      "tensor([ 0.0191,  0.0026,  0.0545,  0.0360, -0.0239, -0.0088,  0.0872,  0.0833,\n",
      "         0.0829,  0.0529,  0.0773, -0.0645,  0.0662,  0.0337,  0.0359, -0.0251,\n",
      "        -0.0668, -0.0347,  0.0814, -0.0637,  0.0242, -0.0025, -0.0755, -0.0890,\n",
      "        -0.0901, -0.0216, -0.0848, -0.0413, -0.0675,  0.0474, -0.0901,  0.0584,\n",
      "         0.0843,  0.0913, -0.0680,  0.0109, -0.0789,  0.0597, -0.0749,  0.0602,\n",
      "         0.0701, -0.0490,  0.0499, -0.0623, -0.0512, -0.0720,  0.0094, -0.0826,\n",
      "        -0.0087,  0.0089, -0.0247,  0.0520, -0.0100, -0.0859, -0.0008,  0.0615,\n",
      "         0.0591,  0.0590, -0.0608, -0.0550, -0.0592, -0.0562, -0.0423, -0.0069,\n",
      "         0.0678,  0.0734,  0.0088,  0.0189,  0.0370, -0.0158, -0.0446,  0.0663,\n",
      "         0.0342,  0.0205, -0.0817,  0.0696,  0.0560, -0.0041, -0.0101,  0.0341,\n",
      "         0.0638, -0.0880,  0.0224, -0.0839], requires_grad=True)), ('fc2.weight', Parameter containing:\n",
      "tensor([[ 0.0531, -0.0549,  0.0000,  ..., -0.0000,  0.0702,  0.0487],\n",
      "        [-0.0580,  0.0000, -0.0661,  ...,  0.0677,  0.0860, -0.0000],\n",
      "        [-0.0000,  0.0750, -0.0000,  ...,  0.0765, -0.0000, -0.0000],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0698,  0.0000,  ...,  0.0528, -0.0817, -0.0893],\n",
      "        [-0.0000, -0.0777, -0.0562,  ..., -0.0443, -0.0593,  0.0687],\n",
      "        [ 0.0668,  0.0000,  0.0503,  ..., -0.0730, -0.0849,  0.0558]],\n",
      "       requires_grad=True)), ('fc3.bias', Parameter containing:\n",
      "tensor([-0.0256,  0.0791,  0.0631,  0.0113, -0.0484,  0.0763,  0.0958, -0.0029,\n",
      "        -0.0168,  0.1015], requires_grad=True)), ('fc3.weight', Parameter containing:\n",
      "tensor([[-0.0480,  0.0833,  0.0750,  0.0000,  0.1047, -0.0675, -0.0000,  0.0529,\n",
      "          0.0788, -0.0665,  0.0948,  0.0598,  0.0994,  0.0000,  0.0817,  0.0503,\n",
      "         -0.0000,  0.0731, -0.0000,  0.1084,  0.0000, -0.0000,  0.0653, -0.0000,\n",
      "          0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,  0.0809,\n",
      "         -0.0000, -0.0000, -0.0809, -0.0424,  0.0000,  0.0000,  0.0000, -0.0436,\n",
      "         -0.0782, -0.0749,  0.0000,  0.0922, -0.0000,  0.0632,  0.0000,  0.0000,\n",
      "          0.0888,  0.0000,  0.0423,  0.0000, -0.0878, -0.0906, -0.0463, -0.0000,\n",
      "         -0.0665, -0.0000,  0.0000, -0.0000, -0.0622, -0.0883,  0.0968,  0.0654,\n",
      "          0.0533, -0.0000,  0.0475,  0.0900,  0.0978,  0.0000,  0.0000, -0.0679,\n",
      "         -0.0561,  0.0980,  0.0000, -0.0536, -0.0847,  0.0965, -0.0000, -0.0548,\n",
      "         -0.0000, -0.1080,  0.0000, -0.0000],\n",
      "        [ 0.0476,  0.1081, -0.0733, -0.0000,  0.1002, -0.0537, -0.0466, -0.1084,\n",
      "          0.0000,  0.0673, -0.0549,  0.1054,  0.0000, -0.1069, -0.0546,  0.0000,\n",
      "          0.0787,  0.0930, -0.0000,  0.0000,  0.0498,  0.1044, -0.0000, -0.0864,\n",
      "         -0.0787,  0.0956, -0.0000,  0.0995, -0.0000, -0.0000, -0.0000,  0.0000,\n",
      "          0.0000, -0.0414, -0.0601,  0.0000,  0.0772,  0.0477,  0.0511, -0.0892,\n",
      "         -0.0618, -0.0450, -0.0668,  0.0799, -0.0505,  0.0000, -0.0000,  0.0000,\n",
      "         -0.0000, -0.0000,  0.1028,  0.0000,  0.0474, -0.0000,  0.0000,  0.0000,\n",
      "          0.0600,  0.0444, -0.0000,  0.0438,  0.0490, -0.0000, -0.0746, -0.0000,\n",
      "          0.0000, -0.0000, -0.1045, -0.0714,  0.0000,  0.0467, -0.1074, -0.0000,\n",
      "         -0.0848, -0.0476,  0.0000,  0.0590,  0.0627,  0.0706, -0.0000, -0.0000,\n",
      "          0.0792,  0.0629, -0.0477, -0.1074],\n",
      "        [-0.0800, -0.0000, -0.0000, -0.0763,  0.0000, -0.0831,  0.1049, -0.0469,\n",
      "         -0.0000, -0.0579,  0.0911, -0.0784,  0.0000, -0.0924, -0.0449,  0.0000,\n",
      "          0.1032,  0.1086, -0.0703,  0.0901,  0.0654,  0.0472, -0.1024, -0.0823,\n",
      "         -0.0521, -0.0782,  0.1040, -0.0598, -0.0895,  0.0000,  0.0929,  0.0771,\n",
      "          0.0000, -0.0000,  0.0000,  0.0857, -0.0000, -0.0000, -0.0000, -0.1035,\n",
      "         -0.0647, -0.0426,  0.0682, -0.0994, -0.0964,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0528,  0.0729,  0.0653,  0.0465, -0.0000,  0.0000,  0.0000,\n",
      "          0.0663,  0.1041, -0.0815, -0.0794,  0.1069,  0.0000, -0.0000,  0.0000,\n",
      "          0.0000, -0.0000,  0.0000,  0.0524, -0.0504,  0.0920, -0.0000,  0.0814,\n",
      "          0.0574, -0.0000,  0.0000, -0.0000,  0.0584, -0.0605, -0.1037, -0.0000,\n",
      "         -0.0000,  0.0000, -0.0746,  0.0605],\n",
      "        [ 0.0000,  0.0753, -0.0000, -0.0453, -0.0000, -0.0884,  0.0659,  0.0438,\n",
      "          0.0463, -0.0783,  0.0787, -0.0454, -0.0507, -0.0726, -0.1006, -0.1071,\n",
      "          0.0000, -0.0000,  0.0000,  0.0653, -0.0932, -0.0646, -0.0779,  0.0000,\n",
      "          0.0781, -0.0000,  0.0000, -0.0000,  0.0000, -0.0479,  0.1007, -0.0440,\n",
      "          0.0000, -0.0664,  0.0494,  0.0774, -0.0913, -0.0000, -0.0000,  0.0000,\n",
      "          0.0446, -0.0740, -0.0886, -0.1069,  0.0000,  0.0646, -0.0526, -0.0000,\n",
      "         -0.0554, -0.0000, -0.0794,  0.0000, -0.0000,  0.0887,  0.0000,  0.0000,\n",
      "         -0.0000, -0.0000, -0.0000,  0.0606,  0.0714, -0.0919, -0.0000, -0.0800,\n",
      "         -0.0000, -0.0887,  0.0547,  0.0000,  0.0000,  0.0000,  0.0894,  0.0000,\n",
      "         -0.0000, -0.0891,  0.0984,  0.0573, -0.0924,  0.0628, -0.0000, -0.0000,\n",
      "         -0.0654, -0.0000,  0.0000,  0.0000],\n",
      "        [-0.0000,  0.0621, -0.0505,  0.0000,  0.0757,  0.0749, -0.0830,  0.0843,\n",
      "         -0.0869,  0.0452, -0.0542,  0.0000, -0.0945,  0.0000, -0.0740,  0.0579,\n",
      "          0.0707, -0.0000, -0.0000,  0.0643, -0.1048,  0.0000, -0.0582,  0.0000,\n",
      "         -0.0752, -0.0662, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0499,\n",
      "          0.0902, -0.0000,  0.0706,  0.0564, -0.0000, -0.0000, -0.0434,  0.0707,\n",
      "          0.0459, -0.0000,  0.0000, -0.0603, -0.0000,  0.0000,  0.0000,  0.0891,\n",
      "         -0.0000, -0.0000,  0.0000,  0.0000, -0.0862, -0.0000, -0.0660, -0.0467,\n",
      "         -0.0716, -0.0601, -0.0849,  0.0000, -0.0000,  0.1014, -0.0000,  0.0000,\n",
      "          0.0000,  0.0541, -0.0570,  0.0801, -0.0505,  0.0814,  0.0000, -0.0953,\n",
      "          0.0626, -0.0430, -0.0000,  0.0675, -0.0953, -0.0000, -0.1067,  0.0633,\n",
      "          0.0421, -0.0877, -0.0609, -0.0000],\n",
      "        [-0.0609,  0.0582, -0.0591,  0.0834,  0.0715, -0.0575,  0.0658, -0.0000,\n",
      "          0.0580, -0.0814, -0.0711, -0.0820,  0.1002,  0.0000, -0.0000, -0.0926,\n",
      "         -0.0881,  0.0000,  0.0000,  0.0000,  0.0701,  0.0000, -0.1053,  0.0547,\n",
      "          0.0000,  0.0833,  0.0936,  0.0828, -0.1060, -0.0000,  0.0441, -0.0000,\n",
      "          0.0000, -0.0854,  0.0000,  0.0473, -0.0507,  0.0920, -0.0706, -0.0000,\n",
      "          0.0567,  0.0715,  0.0779,  0.0000,  0.0000,  0.0976,  0.1028, -0.0994,\n",
      "         -0.1017,  0.0490,  0.0672,  0.0000,  0.1072,  0.0644,  0.0567,  0.0000,\n",
      "         -0.0751, -0.0000,  0.0000,  0.0649,  0.0959, -0.0676, -0.1070,  0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0497, -0.1020, -0.0612, -0.0502,  0.0818,\n",
      "         -0.0000, -0.0979, -0.0000,  0.0832,  0.0702, -0.0000,  0.0000, -0.0653,\n",
      "         -0.0499, -0.0000, -0.0842, -0.0000],\n",
      "        [ 0.0000, -0.0000, -0.0678,  0.0000, -0.0000,  0.0979,  0.0000,  0.0492,\n",
      "         -0.0000,  0.0000, -0.0562,  0.0000, -0.0000,  0.0000,  0.1077,  0.0000,\n",
      "          0.0937,  0.0818,  0.1068, -0.1025,  0.0494, -0.0838,  0.0000,  0.0556,\n",
      "         -0.0000, -0.0000,  0.0000,  0.0777, -0.0481,  0.0000,  0.0000, -0.0777,\n",
      "         -0.0478,  0.0000, -0.0969,  0.0483,  0.0722,  0.0587, -0.0493,  0.0000,\n",
      "          0.0591, -0.0000, -0.1005, -0.1066, -0.1013,  0.0846,  0.1026,  0.0000,\n",
      "          0.0524,  0.0437, -0.0901,  0.0654, -0.0831, -0.0000, -0.0000,  0.1082,\n",
      "          0.0000,  0.0786,  0.0953,  0.0960,  0.0901,  0.0853,  0.0000, -0.0909,\n",
      "         -0.0670, -0.0000, -0.0626,  0.0629, -0.0834, -0.0660, -0.0919, -0.0908,\n",
      "          0.0750, -0.1057, -0.0818,  0.0518, -0.0000, -0.0000,  0.0000,  0.0992,\n",
      "          0.0869, -0.0000, -0.0000,  0.0583],\n",
      "        [ 0.0687,  0.0835,  0.0000, -0.0937, -0.0653, -0.0990, -0.0510,  0.0000,\n",
      "         -0.0705,  0.0854, -0.0888,  0.0000, -0.0000,  0.0996, -0.1075, -0.0851,\n",
      "         -0.0583,  0.0704, -0.0532,  0.0000, -0.0686,  0.0000,  0.1063,  0.0000,\n",
      "         -0.0000,  0.0941,  0.0000,  0.0774,  0.0736, -0.0000, -0.0489, -0.0689,\n",
      "         -0.0455, -0.0950,  0.0000,  0.0641,  0.0758, -0.0820, -0.0000, -0.0000,\n",
      "          0.0000, -0.0724,  0.0000,  0.0919, -0.0497,  0.0000,  0.0000, -0.0000,\n",
      "         -0.0000,  0.0000,  0.0926, -0.0000, -0.0000, -0.0000, -0.0997, -0.0784,\n",
      "         -0.0880, -0.1019,  0.0000, -0.0778,  0.1006, -0.0592,  0.0990,  0.0563,\n",
      "          0.0000,  0.0853, -0.1046,  0.0735, -0.0000,  0.0481, -0.0838, -0.0650,\n",
      "          0.0000, -0.0000, -0.0000,  0.1066, -0.0000,  0.0000,  0.0890, -0.0531,\n",
      "          0.1043,  0.1025,  0.0000,  0.0960],\n",
      "        [-0.0000, -0.0000, -0.0742,  0.0573,  0.0000,  0.0000,  0.0605,  0.0574,\n",
      "         -0.0900, -0.0452,  0.0720,  0.0000, -0.0000,  0.0000, -0.0676,  0.1018,\n",
      "          0.0000, -0.0000, -0.0000, -0.0959,  0.0000, -0.0875,  0.0990,  0.0653,\n",
      "         -0.0544, -0.0915, -0.0513, -0.0000, -0.0000,  0.0528, -0.0000, -0.0000,\n",
      "         -0.0000,  0.1044,  0.1068, -0.0000, -0.0000,  0.0000,  0.0000, -0.1087,\n",
      "          0.0000, -0.0000,  0.0000,  0.0000, -0.0515, -0.0000,  0.0583,  0.0602,\n",
      "         -0.0762,  0.0000, -0.0878,  0.0000,  0.0641,  0.0740,  0.0554,  0.0511,\n",
      "         -0.0964, -0.0420,  0.0000,  0.0975, -0.0000, -0.0957, -0.0856,  0.0652,\n",
      "         -0.0825, -0.0000,  0.0000, -0.0000,  0.0000, -0.0994, -0.0981, -0.0543,\n",
      "          0.0000, -0.0419,  0.0543,  0.0531,  0.0762,  0.0000, -0.0459, -0.0000,\n",
      "          0.0804, -0.0000, -0.0978, -0.0000],\n",
      "        [ 0.0933, -0.0684,  0.0497,  0.0475, -0.0000, -0.0000,  0.0000, -0.1038,\n",
      "          0.0000, -0.0000,  0.0000, -0.0771, -0.0899,  0.0474, -0.0439, -0.0977,\n",
      "          0.0485,  0.0000, -0.0000,  0.0000, -0.0553,  0.0653,  0.0634, -0.0000,\n",
      "         -0.0603, -0.0000, -0.0586,  0.0000,  0.0000,  0.1036, -0.0471, -0.0420,\n",
      "          0.0712,  0.0609,  0.1009,  0.0548, -0.0903,  0.1017, -0.0638, -0.0000,\n",
      "         -0.0727,  0.0425,  0.0479, -0.0000,  0.0891, -0.0965,  0.0000,  0.0471,\n",
      "         -0.0448,  0.0000,  0.0000, -0.0000,  0.1023,  0.0431, -0.0000,  0.0979,\n",
      "         -0.0000,  0.0637,  0.0000, -0.0000, -0.0000,  0.0000,  0.0000, -0.0000,\n",
      "         -0.0000,  0.0975, -0.0930,  0.1065,  0.0765, -0.0711,  0.0866,  0.0000,\n",
      "          0.0676,  0.0773, -0.0704,  0.0669,  0.0788,  0.0844,  0.0416, -0.0986,\n",
      "         -0.0859, -0.0000,  0.0673,  0.0956]], requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "print(list(new_model.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(new_model, \"./save/b.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet()\n",
    "\n",
    "parameters_to_prune = (\n",
    "    (model.conv1, 'weight'),\n",
    "    (model.conv2, 'weight'),\n",
    "    (model.fc1, 'weight'),\n",
    "    (model.fc2, 'weight'),\n",
    "    (model.fc3, 'weight'),\n",
    ")\n",
    "\n",
    "prune.global_unstructured(\n",
    "    parameters_to_prune,\n",
    "    pruning_method=prune.L1Unstructured,\n",
    "    amount=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity in conv1.weight: 1.85%\n",
      "Sparsity in conv2.weight: 7.41%\n",
      "Sparsity in fc1.weight: 22.05%\n",
      "Sparsity in fc2.weight: 12.25%\n",
      "Sparsity in fc3.weight: 10.00%\n",
      "Global sparsity: 20.00%\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Sparsity in conv1.weight: {:.2f}%\".format(\n",
    "        100. * float(torch.sum(model.conv1.weight == 0))\n",
    "        / float(model.conv1.weight.nelement())\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Sparsity in conv2.weight: {:.2f}%\".format(\n",
    "        100. * float(torch.sum(model.conv2.weight == 0))\n",
    "        / float(model.conv2.weight.nelement())\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Sparsity in fc1.weight: {:.2f}%\".format(\n",
    "        100. * float(torch.sum(model.fc1.weight == 0))\n",
    "        / float(model.fc1.weight.nelement())\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Sparsity in fc2.weight: {:.2f}%\".format(\n",
    "        100. * float(torch.sum(model.fc2.weight == 0))\n",
    "        / float(model.fc2.weight.nelement())\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Sparsity in fc3.weight: {:.2f}%\".format(\n",
    "        100. * float(torch.sum(model.fc3.weight == 0))\n",
    "        / float(model.fc3.weight.nelement())\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Global sparsity: {:.2f}%\".format(\n",
    "        100. * float(\n",
    "            torch.sum(model.conv1.weight == 0)\n",
    "            + torch.sum(model.conv2.weight == 0)\n",
    "            + torch.sum(model.fc1.weight == 0)\n",
    "            + torch.sum(model.fc2.weight == 0)\n",
    "            + torch.sum(model.fc3.weight == 0)\n",
    "        )\n",
    "        / float(\n",
    "            model.conv1.weight.nelement()\n",
    "            + model.conv2.weight.nelement()\n",
    "            + model.fc1.weight.nelement()\n",
    "            + model.fc2.weight.nelement()\n",
    "            + model.fc3.weight.nelement()\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending torch.nn.utils.prune with custom pruning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FooBarPruningMethod(prune.BasePruningMethod):\n",
    "    \"\"\"Prune every other entry in a tensor\n",
    "    \"\"\"\n",
    "    PRUNING_TYPE = 'unstructured'\n",
    "\n",
    "    def compute_mask(self, t, default_mask):\n",
    "        mask = default_mask.clone()\n",
    "        mask.view(-1)[::2] = 0\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foobar_unstructured(module, name):\n",
    "    \"\"\"Prunes tensor corresponding to parameter called `name` in `module`\n",
    "    by removing every other entry in the tensors.\n",
    "    Modifies module in place (and also return the modified module)\n",
    "    by:\n",
    "    1) adding a named buffer called `name+'_mask'` corresponding to the\n",
    "    binary mask applied to the parameter `name` by the pruning method.\n",
    "    The parameter `name` is replaced by its pruned version, while the\n",
    "    original (unpruned) parameter is stored in a new parameter named\n",
    "    `name+'_orig'`.\n",
    "\n",
    "    Args:\n",
    "        module (nn.Module): module containing the tensor to prune\n",
    "        name (string): parameter name within `module` on which pruning\n",
    "                will act.\n",
    "\n",
    "    Returns:\n",
    "        module (nn.Module): modified (i.e. pruned) version of the input\n",
    "            module\n",
    "\n",
    "    Examples:\n",
    "        >>> m = nn.Linear(3, 4)\n",
    "        >>> foobar_unstructured(m, name='bias')\n",
    "    \"\"\"\n",
    "    FooBarPruningMethod.apply(module, name)\n",
    "    return module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=84, out_features=10, bias=True)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 0., 1., 0., 1., 0., 1., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "model = LeNet()\n",
    "foobar_unstructured(model.fc3, name='bias')\n",
    "# print(model.fc3.weights_mask)\n",
    "print(model.fc3.bias_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUANTIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# define a floating point model\n",
    "class M(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(M, self).__init__()\n",
    "        self.fc = torch.nn.Linear(4, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# create a model instance\n",
    "model_fp32 = M()\n",
    "# create a quantized model instance\n",
    "model_int8 = torch.quantization.quantize_dynamic(\n",
    "    model_fp32,  # the original model\n",
    "    {torch.nn.Linear},  # a set of layers to dynamically quantize\n",
    "    dtype=torch.qint8)  # the target dtype for quantized weights\n",
    "\n",
    "# run the model\n",
    "input_fp32 = torch.randn(4, 4, 4, 4)\n",
    "res = model_int8(input_fp32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(4,4,4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72.9 µs ± 2.64 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "b = model_fp32(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "\u001b[91m[WARN] Cannot find rule for <class '__main__.M'>. Treat it as zero Macs and zero Params.\u001b[00m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1024.0"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "20.0"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model_fp32, \"./save/a.pt\")\n",
    "flops1, params1 = profile(model_fp32, inputs=(a, ))\n",
    "flops1\n",
    "params1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93.4 µs ± 10 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# run the model\n",
    "input_fp32 = torch.randn(4, 4, 4, 4)\n",
    "res = model_int8(input_fp32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.quantized.modules.linear.LinearPackedParams'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.quantized.dynamic.modules.linear.Linear'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "\u001b[91m[WARN] Cannot find rule for <class '__main__.M'>. Treat it as zero Macs and zero Params.\u001b[00m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model_int8, \"./save/b.pt\")\n",
    "flops2, params2 = profile(model_int8, inputs=(input_fp32, ))\n",
    "flops2\n",
    "params2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M(\n",
       "  (quant): QuantStub()\n",
       "  (conv): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu): ReLU()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wen/.local/lib/python3.8/site-packages/torch/quantization/observer.py:119: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.022613525390625"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.1629, 0.2210, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.6361, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000, 1.2354, 0.0000],\n",
       "          [0.6028, 0.0540, 0.0000, 0.1846],\n",
       "          [0.0000, 0.0000, 1.3695, 0.0000],\n",
       "          [0.1652, 0.0000, 0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.4022, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.7971, 0.5657],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.3361, 0.5855, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.1967, 0.2293],\n",
       "          [0.3332, 0.0000, 0.0000, 0.0000]]]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.0030586719512939453"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.1617, 0.2263, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.6359, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000, 1.2287, 0.0000],\n",
       "          [0.5928, 0.0431, 0.0000, 0.1940],\n",
       "          [0.0000, 0.0000, 1.3473, 0.0000],\n",
       "          [0.1617, 0.0000, 0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.3988, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.7976, 0.5605],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.3341, 0.5820, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.1940, 0.2371],\n",
       "          [0.3341, 0.0000, 0.0000, 0.0000]]]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# define a floating point model where some layers could be statically quantized\n",
    "class M(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(M, self).__init__()\n",
    "        # QuantStub converts tensors from floating point to quantized\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.conv = torch.nn.Conv2d(1, 1, 3,1,1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        # DeQuantStub converts tensors from quantized to floating point\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # manually specify where tensors will be converted from floating\n",
    "        # point to quantized in the quantized model\n",
    "        x = self.quant(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        # manually specify where tensors will be converted from quantized\n",
    "        # to floating point in the quantized model\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "# create a model instance\n",
    "model_fp32 = M()\n",
    "\n",
    "# model must be set to eval mode for static quantization logic to work\n",
    "model_fp32.eval()\n",
    "\n",
    "# attach a global qconfig, which contains information about what kind\n",
    "# of observers to attach. Use 'fbgemm' for server inference and\n",
    "# 'qnnpack' for mobile inference. Other quantization configurations such\n",
    "# as selecting symmetric or assymetric quantization and MinMax or L2Norm\n",
    "# calibration techniques can be specified here.\n",
    "model_fp32.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "\n",
    "# Fuse the activations to preceding layers, where applicable.\n",
    "# This needs to be done manually depending on the model architecture.\n",
    "# Common fusions include `conv + relu` and `conv + batchnorm + relu`\n",
    "model_fp32_fused = torch.quantization.fuse_modules(model_fp32, [['conv', 'relu']])\n",
    "\n",
    "# Prepare the model for static quantization. This inserts observers in\n",
    "# the model that will observe activation tensors during calibration.\n",
    "model_fp32_prepared = torch.quantization.prepare(model_fp32_fused)\n",
    "\n",
    "# calibrate the prepared model to determine quantization parameters for activations\n",
    "# in a real world setting, the calibration would be done with a representative dataset\n",
    "input_fp32 = torch.randn(4, 1, 4, 4)\n",
    "t1 = time.time()\n",
    "r1 = model_fp32_prepared(input_fp32)\n",
    "t2 = time.time()\n",
    "t2 - t1\n",
    "r1\n",
    "# r2 = model_fp32_fused(input_fp32)\n",
    "# r2\n",
    "# Convert the observed model to a quantized model. This does several things:\n",
    "# quantizes the weights, computes and stores the scale and bias value to be\n",
    "# used with each activation tensor, and replaces key operators with quantized\n",
    "# implementations.\n",
    "model_int8 = torch.quantization.convert(model_fp32_prepared)\n",
    "\n",
    "# run the model, relevant calculations will happen in int8\n",
    "t1 = time.time()\n",
    "res = model_int8(input_fp32)\n",
    "t2 = time.time()\n",
    "t2 - t1\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, p in model_int8.named_parameters():\n",
    "    n, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('conv.weight',\n",
       " Parameter containing:\n",
       " tensor([[[[ 0.2498, -0.0528, -0.2379],\n",
       "           [-0.1591,  0.0134,  0.1101],\n",
       "           [ 0.3073, -0.2695, -0.3309]]]], requires_grad=True))"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('conv.bias',\n",
       " Parameter containing:\n",
       " tensor([-0.1820], requires_grad=True))"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for n, p in model_fp32.named_parameters():\n",
    "    n, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M(\n",
       "  (quant): QuantStub()\n",
       "  (conv): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (bn): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wen/.local/lib/python3.8/site-packages/torch/quantization/observer.py:119: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "M(\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8),            quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1,         scale=tensor([1.]), zero_point=tensor([0])\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (conv): ConvBnReLU2d(\n",
       "    1, 1, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (bn): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8),            quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0,         scale=tensor([1.]), zero_point=tensor([0])\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8),            quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1,         scale=tensor([1.]), zero_point=tensor([0])\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (bn): Identity()\n",
       "  (relu): Identity()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wen/.local/lib/python3.8/site-packages/torch/quantization/observer.py:241: UserWarning: must run observer before calling calculate_qparams.                                        Returning default scale and zero point \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]]]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# define a floating point model where some layers could benefit from QAT\n",
    "class M(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(M, self).__init__()\n",
    "        # QuantStub converts tensors from floating point to quantized\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.conv = torch.nn.Conv2d(1, 1, 1)\n",
    "        self.bn = torch.nn.BatchNorm2d(1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        # DeQuantStub converts tensors from quantized to floating point\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "# create a model instance\n",
    "model_fp32 = M()\n",
    "\n",
    "# model must be set to train mode for QAT logic to work\n",
    "model_fp32.train()\n",
    "\n",
    "# attach a global qconfig, which contains information about what kind\n",
    "# of observers to attach. Use 'fbgemm' for server inference and\n",
    "# 'qnnpack' for mobile inference. Other quantization configurations such\n",
    "# as selecting symmetric or assymetric quantization and MinMax or L2Norm\n",
    "# calibration techniques can be specified here.\n",
    "model_fp32.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
    "\n",
    "# fuse the activations to preceding layers, where applicable\n",
    "# this needs to be done manually depending on the model architecture\n",
    "model_fp32_fused = torch.quantization.fuse_modules(model_fp32,\n",
    "    [['conv', 'bn', 'relu']])\n",
    "\n",
    "# Prepare the model for QAT. This inserts observers and fake_quants in\n",
    "# the model that will observe weight and activation tensors during calibration.\n",
    "model_fp32_prepared = torch.quantization.prepare_qat(model_fp32_fused)\n",
    "\n",
    "# run the training loop (not shown)\n",
    "# training_loop(model_fp32_prepared)\n",
    "\n",
    "# Convert the observed model to a quantized model. This does several things:\n",
    "# quantizes the weights, computes and stores the scale and bias value to be\n",
    "# used with each activation tensor, fuses modules where appropriate,\n",
    "# and replaces key operators with quantized implementations.\n",
    "model_fp32_prepared.eval()\n",
    "model_int8 = torch.quantization.convert(model_fp32_prepared)\n",
    "\n",
    "# run the model, relevant calculations will happen in int8\n",
    "res = model_int8(input_fp32)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# thop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_bn() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.Sequential'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "\u001b[91m[WARN] Cannot find rule for <class 'torchvision.models.resnet.Bottleneck'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "\u001b[91m[WARN] Cannot find rule for <class 'torchvision.models.resnet.ResNet'>. Treat it as zero Macs and zero Params.\u001b[00m\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import resnet50\n",
    "from thop import profile\n",
    "model = resnet50()\n",
    "input = torch.randn(1, 3, 224, 224)\n",
    "flops, params = profile(model, inputs=(input, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "\u001b[91m[WARN] Cannot find rule for <class '__main__.YourModule'>. Treat it as zero Macs and zero Params.\u001b[00m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "205960.0"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "60074.0"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class YourModule(nn.Module):\n",
    "    # your definition\n",
    "    def __init__(self):\n",
    "        super(YourModule, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square conv kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5x5 image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, int(x.nelement() / x.shape[0]))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "# def count_your_model(model, x, y):\n",
    "    # your rule here\n",
    "model = YourModule()\n",
    "x = torch.randn(1, 1, 28, 28)\n",
    "# flops, params = profile(model, inputs=(input, ), \n",
    "#                         custom_ops={YourModule: count_your_model})\n",
    "flops, params = profile(model, inputs=(x, ), verbose=True)\n",
    "flops\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thop import clever_format\n",
    "flops, params = clever_format([flops, params], \"%.3f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'205.960K'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'60.074K'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flops\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60074"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 0\n",
    "for name, param in model.named_parameters():\n",
    "    n += param.view(-1).shape[0]\n",
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# freetest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(3,3)\n",
    "a.nelement()\n",
    "a.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-141-ad7aa0f51e8c>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  a = torch.tensor(input)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[5],\n",
       "        [3],\n",
       "        [7],\n",
       "        [1]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = [\n",
    "    [2, 3, 4, 5, 0, 0],\n",
    "    [1, 4, 3, 0, 0, 0],\n",
    "    [4, 2, 2, 5, 7, 0],\n",
    "    [1, 0, 0, 0, 0, 0]\n",
    "]\n",
    "a = torch.tensor(input)\n",
    "#注意index的类型\n",
    "length = torch.LongTensor([[4],[3],[5],[1]])\n",
    "length.shape\n",
    "#index之所以减1,是因为序列维度是从0开始计算的\n",
    "out = torch.gather(a, 1, length-1)\n",
    "out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1],\n",
       "         [2, 3]],\n",
       "\n",
       "        [[4, 5],\n",
       "         [6, 7]]])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(8).reshape(2,2,2)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-161-623c49eea11d>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-161-623c49eea11d>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    b =>1\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# idx = torch.LongTensor([0,1]).reshape(-1,1)\n",
    "idx = torch.randint(0,2,(2,1,2))\n",
    "idx\n",
    "a.shape\n",
    "idx.shape\n",
    "b = torch.gather(a, 1, idx)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 2],\n",
       "        [4, 3, 4]])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([[1, 2], [3, 4]])\n",
    "torch.gather(t, 1, torch.tensor([[0, 0, 1], [1, 0, 1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "219.422px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

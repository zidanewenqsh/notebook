{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec02f03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 01:29:55.319824: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import logging\n",
    "import argparse\n",
    "import shutil\n",
    "import cv2\n",
    "import time\n",
    "import threading\n",
    "import multiprocessing\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from icecream import ic\n",
    "from collections import defaultdict\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04235359",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45d018e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'my': 1,\n",
       " 'i': 2,\n",
       " 'dog': 3,\n",
       " 'love': 4,\n",
       " 'cat': 5,\n",
       " 'hate': 6,\n",
       " 'do': 7,\n",
       " 'you': 8,\n",
       " 'think': 9,\n",
       " 'is': 10,\n",
       " 'amazing': 11}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentense = [\n",
    "    'i love my dog',\n",
    "    'i love my cat',\n",
    "    'i hate my dog',\n",
    "    'do you think my dog is amazing'\n",
    "]\n",
    "tokenizer = Tokenizer(num_words=100)\n",
    "tokenizer.fit_on_texts(sentense)\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5081e363",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentense_seq = tokenizer.texts_to_sequences(sentense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a01ff8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  4,  1,  3,  0,  0,  0,  0,  0,  0],\n",
       "       [ 2,  4,  1,  5,  0,  0,  0,  0,  0,  0],\n",
       "       [ 2,  6,  1,  3,  0,  0,  0,  0,  0,  0],\n",
       "       [ 7,  8,  9,  1,  3, 10, 11,  0,  0,  0]], dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paded_sequence = pad_sequences(sentense_seq, padding='post',maxlen=10)\n",
    "paded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61299c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  4,  1,  3,  0,  0],\n",
       "       [ 2,  4,  1,  5,  0,  0],\n",
       "       [ 2,  6,  1,  3,  0,  0],\n",
       "       [ 8,  9,  1,  3, 10, 11]], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paded_sequence = pad_sequences(sentense_seq, padding='post',maxlen=6)\n",
    "paded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42612177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  4,  1,  3,  0,  0],\n",
       "       [ 2,  4,  1,  5,  0,  0],\n",
       "       [ 2,  6,  1,  3,  0,  0],\n",
       "       [ 7,  8,  9,  1,  3, 10]], dtype=int32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paded_sequence = pad_sequences(sentense_seq, padding='post',maxlen=6, truncating='post')\n",
    "paded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "818d5090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  4,  1,  3, -1, -1],\n",
       "       [ 2,  4,  1,  5, -1, -1],\n",
       "       [ 2,  6,  1,  3, -1, -1],\n",
       "       [ 7,  8,  9,  1,  3, 10]], dtype=int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paded_sequence = pad_sequences(sentense_seq, padding='post',maxlen=6, truncating='post', value=-1)\n",
    "paded_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13e355a",
   "metadata": {},
   "source": [
    "### stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f9920cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stopwords_list(stop_path):\n",
    "    with open(stop_path, encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    return [line.strip() for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6edd3193",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = get_stopwords_list(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f8bc5fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_stopwords_list(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd9d19e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_stopwords(sentence_list, stopwords_list):\n",
    "    out_list = []\n",
    "#     stopwords_list.append('\\t')\n",
    "    for word in sentence_list:\n",
    "        if word in stopwords_list:continue\n",
    "        out_list.append(word)\n",
    "    return out_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd66eacd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['my', 'i', 'dog', 'love', 'cat', 'hate', 'do', 'you', 'think', 'is', 'amazing'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index.keys()\n",
    "# tokenizer.word_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "47bd4179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dog', 'love', 'cat', 'hate', 'think', 'amazing']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "move_stopwords(tokenizer.word_index.keys(), stopwords_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcdab80",
   "metadata": {},
   "source": [
    "#### nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5ec6f999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting nltk\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/43/0b/8298798bc5a9a007b7cae3f846a3d9a325953e0f9c238affa478b4d59324/nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from nltk) (4.61.2)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk) (8.0.3)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/82/b9/09143a2072af5571227f1687e44fd9041cc5933fffaf2fbc30394c720141/regex-2022.1.18-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (748 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m749.0/749.0 KB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click->nltk) (4.10.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (3.10.0.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (3.7.0)\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.7 regex-2022.1.18\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !pip install nltk -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "699265bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c32878d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/bin/python\r\n"
     ]
    }
   ],
   "source": [
    "# !which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c4908db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "de503624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "247ec648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation = string.punctuation\n",
    "punctuation.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8a138177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hasn',\n",
       " 'or',\n",
       " 'where',\n",
       " ')',\n",
       " \"you'll\",\n",
       " 'whom',\n",
       " 'that',\n",
       " 'shan',\n",
       " '#',\n",
       " \"that'll\",\n",
       " ',',\n",
       " 'y',\n",
       " 'it',\n",
       " 'once',\n",
       " 'were',\n",
       " 'haven',\n",
       " ';',\n",
       " 'how',\n",
       " '\\\\',\n",
       " 'against',\n",
       " 'there',\n",
       " 'because',\n",
       " 'don',\n",
       " 'up',\n",
       " \"don't\",\n",
       " 'ours',\n",
       " 'i',\n",
       " 'here',\n",
       " \"weren't\",\n",
       " 'his',\n",
       " 'does',\n",
       " '%',\n",
       " 'through',\n",
       " ']',\n",
       " \"shan't\",\n",
       " \"didn't\",\n",
       " 'they',\n",
       " 'ain',\n",
       " 'll',\n",
       " 'other',\n",
       " 'to',\n",
       " 'very',\n",
       " 'before',\n",
       " 'being',\n",
       " 'won',\n",
       " 'he',\n",
       " 'about',\n",
       " '`',\n",
       " 'further',\n",
       " '|',\n",
       " 'did',\n",
       " 'those',\n",
       " 'who',\n",
       " 'an',\n",
       " 'when',\n",
       " 'each',\n",
       " \"'\",\n",
       " 'no',\n",
       " 'doesn',\n",
       " \"haven't\",\n",
       " 'which',\n",
       " 'then',\n",
       " 'are',\n",
       " 'hadn',\n",
       " 'on',\n",
       " 'shouldn',\n",
       " 'both',\n",
       " \"wouldn't\",\n",
       " '!',\n",
       " 'can',\n",
       " 's',\n",
       " 'wasn',\n",
       " 'most',\n",
       " \"mustn't\",\n",
       " 'as',\n",
       " 'do',\n",
       " 'mustn',\n",
       " 'itself',\n",
       " 'weren',\n",
       " 'wouldn',\n",
       " '}',\n",
       " 'themselves',\n",
       " '$',\n",
       " \"it's\",\n",
       " 've',\n",
       " 'out',\n",
       " 'o',\n",
       " 'down',\n",
       " 'all',\n",
       " 'of',\n",
       " 'our',\n",
       " 'yours',\n",
       " '/',\n",
       " 'own',\n",
       " 'me',\n",
       " 'under',\n",
       " 'ma',\n",
       " 'what',\n",
       " 'them',\n",
       " 'had',\n",
       " 'now',\n",
       " 'again',\n",
       " '@',\n",
       " 'in',\n",
       " 'with',\n",
       " '(',\n",
       " 'below',\n",
       " 'she',\n",
       " 'myself',\n",
       " '=',\n",
       " \"wasn't\",\n",
       " 'its',\n",
       " 'why',\n",
       " \"needn't\",\n",
       " 'any',\n",
       " '[',\n",
       " 'couldn',\n",
       " 'been',\n",
       " '~',\n",
       " 'having',\n",
       " 'will',\n",
       " 're',\n",
       " 'needn',\n",
       " 'over',\n",
       " 'your',\n",
       " 'few',\n",
       " 'this',\n",
       " 'nor',\n",
       " 'these',\n",
       " \"hasn't\",\n",
       " '+',\n",
       " '-',\n",
       " '<',\n",
       " 'm',\n",
       " \"should've\",\n",
       " 'between',\n",
       " 'yourselves',\n",
       " 'their',\n",
       " 'during',\n",
       " 'into',\n",
       " 'yourself',\n",
       " 'same',\n",
       " 'but',\n",
       " '?',\n",
       " 'just',\n",
       " '\\t',\n",
       " 'am',\n",
       " \"aren't\",\n",
       " 'isn',\n",
       " 'herself',\n",
       " 'didn',\n",
       " 'her',\n",
       " 'a',\n",
       " 'is',\n",
       " 'while',\n",
       " \"she's\",\n",
       " \"won't\",\n",
       " 'above',\n",
       " '&',\n",
       " 'has',\n",
       " ':',\n",
       " \"you've\",\n",
       " 'for',\n",
       " \"doesn't\",\n",
       " 'ourselves',\n",
       " 't',\n",
       " 'only',\n",
       " 'not',\n",
       " 'my',\n",
       " 'the',\n",
       " 'off',\n",
       " 'such',\n",
       " 'if',\n",
       " '>',\n",
       " 'at',\n",
       " \"you're\",\n",
       " 'd',\n",
       " 'be',\n",
       " 'have',\n",
       " 'aren',\n",
       " 'by',\n",
       " 'should',\n",
       " '_',\n",
       " 'and',\n",
       " 'you',\n",
       " 'was',\n",
       " '*',\n",
       " \"you'd\",\n",
       " \"hadn't\",\n",
       " 'doing',\n",
       " \"shouldn't\",\n",
       " \"couldn't\",\n",
       " 'than',\n",
       " 'hers',\n",
       " 'theirs',\n",
       " 'too',\n",
       " '{',\n",
       " 'until',\n",
       " 'more',\n",
       " 'from',\n",
       " 'him',\n",
       " 'so',\n",
       " 'after',\n",
       " \"mightn't\",\n",
       " 'mightn',\n",
       " '.',\n",
       " 'we',\n",
       " 'himself',\n",
       " '^',\n",
       " '\"',\n",
       " \"isn't\",\n",
       " 'some']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = set(stopwords_list)\n",
    "stop.update(set(punctuation))\n",
    "list(stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c917a0",
   "metadata": {},
   "source": [
    "#### jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "95013a0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting jieba\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/c6/cb/18eeb235f833b726522d7ebed54f2278ce28ba9438e3135ab0278d9792a2/jieba-0.42.1.tar.gz (19.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: jieba\n",
      "  Building wheel for jieba (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314477 sha256=4fb5ff6a8669450ed170677df09bd8a339ad05c2d3d5a1e12c368811e492c015\n",
      "  Stored in directory: /root/.cache/pip/wheels/95/1a/6d/75355e7a5c76ed48e2d6cde3b95c4828e83274b93f5392ac96\n",
      "Successfully built jieba\n",
      "Installing collected packages: jieba\n",
      "Successfully installed jieba-0.42.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !pip install jieba -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ac9d7e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e4c91caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.715 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['中国', '是', '最', '强大', '的']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentense2 = '中国是最强大的'\n",
    "s = jieba.cut(sentense2)\n",
    "list(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cc331e",
   "metadata": {},
   "source": [
    "http://aimaksen.bslience.cn/bbc-text.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4dfee525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mSaves\u001b[0m/    bbc-text.csv  test01.ipynb  test03.ipynb  test05.ipynb  test9.ipynb\r\n",
      "\u001b[01;34mSources\u001b[0m/  english       test02.ipynb  test04.ipynb  test08.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a37d6dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cp /tmp/bbc-text.csv .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b485c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

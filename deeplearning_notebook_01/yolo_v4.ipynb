{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell \n",
    "InteractiveShell.ast_node_interactivity = 'all' #默认为'last'\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self,x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(451, 338)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_file = r\"./datas/cat.jpg\"\n",
    "img = Image.open(img_file)\n",
    "# img.show()\n",
    "img.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = transforms.Compose([transforms.Resize([50, 50])])\n",
    "# transformer = transforms.Compose([transforms.ToTensor()])\n",
    "img_ = transformer(img)\n",
    "img_.size\n",
    "img_.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3059, 0.3020, 0.2941,  ..., 0.3686, 0.3804, 0.3922],\n",
       "         [0.3059, 0.3137, 0.3098,  ..., 0.3922, 0.4078, 0.4157],\n",
       "         [0.3137, 0.3098, 0.3059,  ..., 0.4078, 0.4118, 0.4235],\n",
       "         ...,\n",
       "         [0.8118, 0.8157, 0.8157,  ..., 0.8431, 0.8431, 0.8431],\n",
       "         [0.8275, 0.8275, 0.8275,  ..., 0.8314, 0.8235, 0.8275],\n",
       "         [0.8235, 0.8275, 0.8275,  ..., 0.8157, 0.8157, 0.8157]],\n",
       "\n",
       "        [[0.4667, 0.4588, 0.4588,  ..., 0.4980, 0.5137, 0.5255],\n",
       "         [0.4745, 0.4627, 0.4588,  ..., 0.5176, 0.5255, 0.5373],\n",
       "         [0.4745, 0.4706, 0.4706,  ..., 0.5255, 0.5333, 0.5451],\n",
       "         ...,\n",
       "         [0.8784, 0.8784, 0.8784,  ..., 0.8863, 0.8824, 0.8824],\n",
       "         [0.8902, 0.8863, 0.8863,  ..., 0.8784, 0.8784, 0.8745],\n",
       "         [0.8824, 0.8902, 0.8941,  ..., 0.8745, 0.8745, 0.8745]],\n",
       "\n",
       "        [[0.6392, 0.6314, 0.6275,  ..., 0.6431, 0.6510, 0.6588],\n",
       "         [0.6392, 0.6353, 0.6353,  ..., 0.6471, 0.6588, 0.6667],\n",
       "         [0.6431, 0.6392, 0.6314,  ..., 0.6510, 0.6588, 0.6706],\n",
       "         ...,\n",
       "         [0.8745, 0.8784, 0.8784,  ..., 0.8824, 0.8784, 0.8745],\n",
       "         [0.8902, 0.8863, 0.8902,  ..., 0.8784, 0.8745, 0.8745],\n",
       "         [0.8824, 0.8863, 0.8902,  ..., 0.8745, 0.8706, 0.8745]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer = transforms.Compose([transforms.Resize([50, 50]),transforms.ToTensor()])\n",
    "img_ = transformer(img)\n",
    "img_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=50x50 at 0x2A7329C4F08>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_1 = transforms.ToPILImage()(img_)\n",
    "img_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 2., 3.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([4., 2., 3.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.Tensor([1,2,3])\n",
    "b = a\n",
    "a[0]=4\n",
    "a\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.12"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "32.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "42.24"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "64.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "84.48"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "96.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 0.33\n",
    "d = 32\n",
    "for i in [64,128,256]:\n",
    "    a = np.ceil(i*n/d) * d\n",
    "    i * n\n",
    "    a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3],\n",
       "        [4, 5],\n",
       "        [6, 7],\n",
       "        [8, 9]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([0, 2])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(10).reshape(-1,2)\n",
    "a\n",
    "a[a[:,0]>10].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.38129"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16.88129+0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.1187100000000001"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16.88129-17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(10)\n",
    "b = torch.randn(3,3)\n",
    "c = a.type(b.dtype)\n",
    "c\n",
    "c.dtype\n",
    "a.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1.]), tensor([1.]), tensor([1.])]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [torch.Tensor([1.0])]\n",
    "a*=3\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2727272727272727"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "ValueError",
     "evalue": "cannot resize an array that references or is referenced\nby another array in this way.\nUse the np.resize function or refcheck=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-167a6d86a553>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: cannot resize an array that references or is referenced\nby another array in this way.\nUse the np.resize function or refcheck=False"
     ]
    }
   ],
   "source": [
    "a = np.arange(10)\n",
    "# b = np.resize(a,(5,))\n",
    "a\n",
    "# b\n",
    "a.resize((2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7578125"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3600/2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0 and 0 or 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3],\n",
       "        [4, 5]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[0, 1],\n",
       "          [2, 3],\n",
       "          [4, 5]],\n",
       "\n",
       "         [[0, 1],\n",
       "          [2, 3],\n",
       "          [4, 5]]],\n",
       "\n",
       "\n",
       "        [[[0, 1],\n",
       "          [2, 3],\n",
       "          [4, 5]],\n",
       "\n",
       "         [[0, 1],\n",
       "          [2, 3],\n",
       "          [4, 5]]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(6).reshape(3, 2)\n",
    "a\n",
    "b = a.repeat(2,2,1,1)\n",
    "b.shape\n",
    "b\n",
    "# torch.repeat(a ,1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ff872723d10>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1268,  1.3564],\n",
       "         [-0.0247, -0.8466],\n",
       "         [ 0.0293, -0.5721]],\n",
       "\n",
       "        [[-1.2546,  0.0486],\n",
       "         [ 0.2753, -2.1550],\n",
       "         [-0.7116,  0.0575]],\n",
       "\n",
       "        [[ 0.6263, -1.7736],\n",
       "         [-0.2205,  2.7467],\n",
       "         [-1.0480,  1.1239]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[  7.8834,   0.7372],\n",
       "         [-40.5364,  -1.1812],\n",
       "         [ 34.1282,  -1.7480]],\n",
       "\n",
       "        [[ -0.7970,  20.5715],\n",
       "         [  3.6323,  -0.4640],\n",
       "         [ -1.4054,  17.3835]],\n",
       "\n",
       "        [[  1.5968,  -0.5638],\n",
       "         [ -4.5351,   0.3641],\n",
       "         [ -0.9542,   0.8897]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 7.8834e+00,  1.3564e+00],\n",
       "         [-2.4669e-02, -8.4661e-01],\n",
       "         [ 3.4128e+01, -5.7207e-01]],\n",
       "\n",
       "        [[-7.9705e-01,  2.0572e+01],\n",
       "         [ 3.6323e+00, -4.6405e-01],\n",
       "         [-7.1156e-01,  1.7383e+01]],\n",
       "\n",
       "        [[ 1.5968e+00, -5.6384e-01],\n",
       "         [-2.2050e-01,  2.7467e+00],\n",
       "         [-9.5421e-01,  1.1239e+00]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([[ 7.8834e+00, -2.4669e-02,  3.4128e+01],\n",
       "        [ 2.0572e+01,  3.6323e+00,  1.7383e+01],\n",
       "        [ 1.5968e+00,  2.7467e+00,  1.1239e+00]]),\n",
       "indices=tensor([[0, 0, 0],\n",
       "        [1, 0, 1],\n",
       "        [0, 1, 1]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.8834e+00, -2.4669e-02,  3.4128e+01],\n",
       "        [ 2.0572e+01,  3.6323e+00,  1.7383e+01],\n",
       "        [ 1.5968e+00,  2.7467e+00,  1.1239e+00]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(100)\n",
    "r = torch.randn(3,3,2)\n",
    "r\n",
    "1/r\n",
    "j1 = torch.max(r, 1. / r)\n",
    "j1\n",
    "j1.max(2)\n",
    "j = torch.max(r, 1. / r).max(2)[0]\n",
    "j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9291,  1.0564, -0.1727],\n",
       "        [ 0.4991,  0.5274,  1.1405],\n",
       "        [-0.4631,  0.5797, -1.3473]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.9291, 0.0564, 0.8273],\n",
       "        [0.4991, 0.5274, 0.1405],\n",
       "        [0.5369, 0.5797, 0.6527]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(3,3)\n",
    "a\n",
    "a % 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mod(100,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "107.5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top,bottom,left,right: (107, 108, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "dw,dh=0, 215\n",
    "dw /=2\n",
    "dh /=2\n",
    "dw\n",
    "dh\n",
    "top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "if 1:\n",
    "    print(f\"top,bottom,left,right: {top,bottom,left,right}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('a', 1), ('b', 2)])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "dict_items([('a', 1), ('b', 2)])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {'a':1,'b':2}\n",
    "a.items()\n",
    "for k,v in a.items():\n",
    "    v+=1\n",
    "a.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('a', tensor([1.])), ('b', 2)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "dict_items([('a', tensor([2.])), ('b', 2)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {'a':torch.Tensor([1]),'b':2}\n",
    "a.items()\n",
    "for k,v in a.items():\n",
    "    v+=1\n",
    "a.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.018322666666666664"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.01849*29+0.01347)/30"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
